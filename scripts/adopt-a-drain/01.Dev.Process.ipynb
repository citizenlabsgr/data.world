{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Adopt a Drain Data\n",
    " * Author: James Wilfong, wilfongjt@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goals\n",
    "* Merge drains from multiple communities across the watershed\n",
    "* Resolve dataset conflicts i.e., drain identifyer uniqueness, data type, column names, and value range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limits\n",
    " * Outputs from this script need to be manually pushed to the data.world repository (repo)\n",
    " * Doesn't handle deleted drains\n",
    " \n",
    "## Assumptions\n",
    "* Assume the community-dataset is a subset of all drains in the universe ;)\n",
    "* Assume record identifiers are not unique across communities \n",
    "* Assume the community-dataset's column names are not the same across communities \n",
    "* Assume the community-dataset's data types are not the same across communities\n",
    "* Assume the community-dataset's data-values ranges are not the same across communities\n",
    "* Assume the community-dataset has duplicate records\n",
    "* Assume the community-dataset's format is Comma Seperated Values (CSV) or Excel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs\n",
    " * The current live dataset is downloaded from data.world \n",
    " * Updates are put into the raw-data/adopt-a-drain of the data.world repo.\n",
    "\n",
    "## Process\n",
    "  The process is initiated by running this Jupyter Notebook.\n",
    " * load current drains dataset from data.world\n",
    " * load dataset(s) from raw-data/adopt-a-drain folder\n",
    " * clean data: create unique id from facility id etal.\n",
    " * clean data: remove characters from facility ids\n",
    " * clean data: map input columns to expected output columns\n",
    " * clean data: fix common data problems\n",
    " * condense data: remove drains with no facility id\n",
    " * condense data: remove unneeded columns\n",
    " * condense data: remove outliers\n",
    " * condense: merge dataworld and new data \n",
    " * condense data: remove duplicate drains (keep the first duplicate)\n",
    " * concat: Make one big dataset from one or many\n",
    "\n",
    "## Outputs\n",
    " * Clean data is output to the clean-data/adopt-a-drain folder.\n",
    " * save big dataset to clean-data/adopt-a-drain/grb_drains.csv\n",
    " * save big dataset to clean-data/grb_drains-2019-08-020.csv\n",
    " * \n",
    " \n",
    "## Next Steps\n",
    " * Push updates to repo\n",
    " * Sync data from github to dataworld\n",
    "     * Sync Manually\n",
    "     * or Wait for the weekly auto-sync\n",
    " * Update the Adopt a Drain database\n",
    "     * Run Ruby rake process in Heroku "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "import settings\n",
    "from IPython.display import display, HTML\n",
    "from IPython.display import Markdown\n",
    "from lib.p3_ProcessLogger import ProcessLogger\n",
    "\n",
    "import helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "from os import path\n",
    "import sys\n",
    "import time\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import csv # read and write csv files\n",
    "from pprint import pprint\n",
    "import os\n",
    "\n",
    "import datadotworld as dw\n",
    "from shutil import copyfile\n",
    "# import subprocess\n",
    "\n",
    "# convenience functions -- cleaning\n",
    "# cell_log.collect('* Import custom packages')\n",
    "from lib.p3_CellCounts import CellCounts\n",
    "# import lib.p3_clean as clean\n",
    "from lib.p3_configuration import get_configuration\n",
    "import lib.p3_explore as explore\n",
    "#import lib.p3_gather as gather # gathering functions\n",
    "# import lib.p3_helper_functions as helper\n",
    "import lib.p3_map as maps\n",
    "from lib.p3_ProcessLogger import ProcessLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTANT\n",
    "## Configuring the Data Transfer\n",
    "Configure before running \"RUN All\" in the Cell menu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kill log:  ./maintainer/maintainer-config.json\n",
      "Metadata =============\n",
      "{'copy_file_name': 'grb_drains.csv',\n",
      " 'desc': 'Storm Drains of the Grand River Basin, Michigan',\n",
      " 'gh_file_type': 'csv',\n",
      " 'output_file_name': 'grb_drains-2019-09-026.csv',\n",
      " 'table_name': 'grb_drains',\n",
      " 'title': 'GRB Storm Drains'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "'''\n",
    "    Input: CSV file in raw_data/ folder\n",
    "    Process: clean (conform, condence)\n",
    "    Output: is directed to the clean-data/ folder\n",
    "    \n",
    "    Name the output file using OUTPUT_FILE\n",
    "    OUTPUT_FILELOCAL_CLEAN_NAME is used to name the data.world table\n",
    "    Table names should start with letter, may contain letters, numbers, underscores\n",
    "    \n",
    "'''\n",
    "cell_log = ProcessLogger() \n",
    "cell_log.clear()\n",
    "table_name='grb_drains'\n",
    "\n",
    "metadata = {\n",
    "    'output_file_name': '{}-{}.csv'.format(table_name, helper.get_daystamp(),'csv'),\n",
    "    'copy_file_name': '{}.csv'.format(table_name),\n",
    "    'gh_file_type': 'csv',\n",
    "    'title': 'GRB Storm Drains',\n",
    "    'desc': 'Storm Drains of the Grand River Basin, Michigan',\n",
    "    'table_name': table_name\n",
    "}\n",
    "\n",
    "helper.exportMaintainerConfig(metadata['output_file_name'], \n",
    "                              metadata['gh_file_type'], \n",
    "                              metadata['title'], \n",
    "                              metadata['desc'], \n",
    "                              metadata['table_name'])    \n",
    "print('Metadata =============')\n",
    "pprint(metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local_config ===========\n",
      "{'app_name': 'adopt-a-drain', 'local_clean': None, 'local_raw': None}\n",
      "outlier_settings ===========\n",
      "{'outliers': [{'column': 'dr_lon',\n",
      "               'count': 0,\n",
      "               'range': (-90.0, -80.0),\n",
      "               'reason': 'Remove {} observations too far west or east.'},\n",
      "              {'column': 'dr_lat',\n",
      "               'count': 0,\n",
      "               'range': (40.0, 50.0),\n",
      "               'reason': 'Remove {} observations too far north or south.'}]}\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Assemble Names of:\n",
    "        Application,\n",
    "        Raw data file,\n",
    "        Clean data file\n",
    "'''\n",
    "\n",
    "local_config = { \n",
    "                 \"app_name\": helper.get_app_name(),\n",
    "                 \"local_raw\": None,\n",
    "                 \"local_clean\": None\n",
    "               }\n",
    "\n",
    "'''\n",
    "    ------------- configure outliers\n",
    "'''\n",
    "# _outliers = {\n",
    "outlier_settings = {\n",
    "  'outliers': [\n",
    "    {'column':'dr_lon',\n",
    "     'range':(-90.0, -80.0),\n",
    "     'reason':'Remove {} observations too far west or east.',\n",
    "     'count': 0\n",
    "    },  \n",
    "    {'column':'dr_lat',\n",
    "     'range':(40.0, 50.0),\n",
    "     'reason':'Remove {} observations too far north or south.',\n",
    "     'count': 0\n",
    "    }\n",
    "  ]\n",
    "}  \n",
    "ENV_ERROR=False\n",
    "print(\"local_config ===========\")\n",
    "pprint(local_config)\n",
    "print(\"outlier_settings ===========\")\n",
    "pprint(outlier_settings)\n",
    "#print('Local_RAW_FILE: ', LOCAL_RAW_FILE)\n",
    "#print('LOCAL_CLEAN_FILE: ',LOCAL_CLEAN_FILE )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 724,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if ENV_ERROR:\n",
    "    cell_log.collect(\"# Script Failure!!\")\n",
    "    cell_log.collect(\"# !!! Missing Environment Variables !!!\")\n",
    "    cell_log.collect(\"### see [Environment Variable Setup](#env-setup)\")\n",
    "\n",
    "Markdown('''{}'''.format(cell_log.getMarkdown()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common names from imported files and how then map to actual names\n",
    "\n",
    "maps ={\n",
    "    \"commonNameMap\": { \n",
    "        \"subtype\": \"dr_subtype\",\n",
    "        \"jurisdicti\": \"dr_jurisdiction\",\n",
    "        \"drain__owner\": \"dr_owner\",\n",
    "        \"owner\":\"dr_owner\",\n",
    "        \"local__id\": \"dr_local_id\",\n",
    "        \"facilityid\": \"dr_facility_id\",\n",
    "        \"drain__jurisdiction\": \"dr_jurisdiction\",\n",
    "        \"subwatershed\": \"dr_subwatershed\",\n",
    "        \"subbasin\": \"dr_subwatershed\",\n",
    "        \"point__x\":\"dr_lon\", \n",
    "        \"long\": \"dr_lon\",\n",
    "        \"point__y\":\"dr_lat\",\n",
    "        \"lat\":\"dr_lat\",\n",
    "        \"soure__id\": \"del_source_id\"\n",
    "    },\n",
    "\n",
    "    \"region_map\": {\n",
    "        \"Kent County Road Commission\": \"KCRC\",\n",
    "        \"KENT COUNTY ROAD COMMISSION\":\"KCRC\",\n",
    "        \"City of East Grand Rapids\": \"EGR\",\n",
    "        \"City of Grandville\": \"GRANDV\",\n",
    "        \"City of Wyoming\": \"CWY\",\n",
    "        \"City of Kentwood\": \"CK\",\n",
    "        \"Grand Rapids Township\": \"GRTWP\",\n",
    "        \"City of Walker\": \"CW\",\n",
    "        \"CGR\": \"CGR\",\n",
    "        \"City of Grand Rapids\": \"CGR\",\n",
    "        \"Georgetown Township\": \"GTWP\",\n",
    "        \"City of Hudsonville\": \"CHV\",\n",
    "        \"Jamestown Township\": \"JTTWP\",\n",
    "        \"Cascade Township\": \"CASTWP\",\n",
    "        \"Algoma Township\": \"ALGTWP\",\n",
    "        \"Grattan Township\": \"GRATWP\",\n",
    "        \"Gaines Township\": \"GAITWP\",\n",
    "        \"Vergennes Township\": \"VERTWP\",\n",
    "        \"Lowell Township\": \"LOWTWP\",\n",
    "        \"Oakfield Township\": \"OAKTWP\",\n",
    "        \"Cannon Township\": \"CANTWP\",\n",
    "        \"Sparta Township\": \"SPATWP\",\n",
    "        \"Solon Township\": \"SOLTWP\",\n",
    "        \"Ada Township\": \"ADATWP\",\n",
    "        \"City of Lowell\": \"CLO\",\n",
    "        \"Bowne Township\": \"BOWTWP\",\n",
    "        \"Tyrone Township\": \"TYRTWP\",\n",
    "        \"Caledonia Township\": \"CALTWP\",\n",
    "        \"Courtland Township\": \"COUTWP\",\n",
    "        \"Spencer Township\": \"SPETWP\",\n",
    "        \"Village of Sparta\": \"VSP\",\n",
    "        \"BYRON TOWNSHIP\": \"BYRTWP\",\n",
    "        \"CALEDONIA TOWNSHIP\": \"CALETWP\",\n",
    "        \"City of Rockford\": \"CRF\",\n",
    "        \"Alpine Township\": \"ALPTWP\",\n",
    "        \"Plainfield Township\": \"PLATWP\",\n",
    "        \"Byron Township\": \"BYRTWP\",\n",
    "        \"OCWRC\": \"OCWRC\",\n",
    "        \"City of Grand Haven DPW\":\"CGH\",\n",
    "        \"Village of Spring Lake DPW\": \"VSL\",\n",
    "        \"Ottawa County Road Commission\": \"OCRC\",\n",
    "        \"OCRC\": \"OCRC\",\n",
    "        \"Ottawa County Water Resource Commissioner\":\"OCRC\",\n",
    "        \"Village of Fruitport\":\"VF\"\n",
    "    }\n",
    "}\n",
    "# if you add a temporary column, then add to this list remove before saving \n",
    "extraColumns = ['del_source',\n",
    "              'del_fid',\n",
    "              'del_gid',\n",
    "              'source_code',\n",
    "              'dr_local_id',\n",
    "              'dr_facility_id',\n",
    "              'dr_location',\n",
    "              'dup_coordinate']\n",
    "# \n",
    "expected_process_columns_list = ['facility_prefix', \n",
    "                         'dr_subtype', \n",
    "                         'dr_jurisdiction', \n",
    "                         'dr_owner', \n",
    "                         'dr_subwatershed', \n",
    "                         'dr_lon', \n",
    "                         'dr_lat', \n",
    "                         'dr_asset_id', \n",
    "                         'dr_type', \n",
    "                         'dr_sync_id',\n",
    "                         'dr_discharge']\n",
    "\n",
    "# add column \n",
    "\n",
    "expected_output_columns_list= ['dr_asset_id',\n",
    "                              'dr_jurisdiction',\n",
    "                              'dr_lat',\n",
    "                              'dr_lon',\n",
    "                              'dr_owner',\n",
    "                              'dr_subtype',\n",
    "                              'dr_subwatershed',\n",
    "                              'dr_type',\n",
    "                              'dr_discharge' \n",
    "                             ]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Process():\n",
    "    def __init__(self):\n",
    "        self.summary={} # stash process results, counts, here\n",
    "        self.logger = ProcessLogger('./logs/{}.log'.format(self.getClassName()))\n",
    "        \n",
    "    def getClassName(self):\n",
    "        return self.__class__.__name__\n",
    "    \n",
    "    def getLogger(self):\n",
    "        return self.logger\n",
    "    \n",
    "    def getSummary(self):\n",
    "        return self.summary\n",
    "    \n",
    "    def setSummary(self, summary):\n",
    "        self.summary = summary\n",
    "        return self\n",
    "    \n",
    "    def filename(self, in_f):\n",
    "        ps = in_f.split('/')\n",
    "        return ps[len(ps)-1]\n",
    "    \n",
    "    def process(self):\n",
    "        raise Exception('Overload process() in {}'.format(self.getClassName())) \n",
    "        \n",
    "    def run(self):\n",
    "        self.process()\n",
    "        return self   \n",
    "\n",
    "class Load(Process):\n",
    "    def __init__(self, import_file_name):\n",
    "        Process.__init__(self)\n",
    "        # import_file_name is  full local file name or url to source\n",
    "        self.import_file_name=import_file_name\n",
    "        self.dataframe=None\n",
    "        \n",
    "    def set_dataframe(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "    \n",
    "    def get_dataframe(self):\n",
    "        return self.dataframe\n",
    "        \n",
    "class LoadDrains(Load):\n",
    "    def __init__(self, import_file_name):\n",
    "        Load.__init__(self,import_file_name)\n",
    "        # self.import_file_name=import_file_name\n",
    "        self.summary_key='02'\n",
    "    \n",
    "    def get_class_key(self):\n",
    "        return '{}.{}'.format(self.summary_key, self.getClassName())\n",
    "    \n",
    "    def get_app_name(self):\n",
    "        '''\n",
    "        returns application name from script path\n",
    "        '''\n",
    "        scripts_path = os.getcwd()\n",
    "        rc = ''\n",
    "        pth = scripts_path.split('/')\n",
    "        rc = pth[len(pth)-1]\n",
    "        return rc\n",
    "\n",
    "    def get_repo_folder(self):\n",
    "        '''\n",
    "        returns path to the repo folder from script path\n",
    "        '''\n",
    "        scripts_path = os.getcwd()\n",
    "        rc = ''\n",
    "        rc = scripts_path.replace('/' + self.get_app_name(), '').replace('/scripts','')\n",
    "        return rc\n",
    "    \n",
    "    def get_raw_data_folder(self):\n",
    "        '''\n",
    "        returns path to raw data from script path\n",
    "        '''\n",
    "        scripts_path = os.getcwd()\n",
    "        return self.get_repo_folder() + '/raw-data/' + self.get_app_name()\n",
    "    \n",
    "    def filename(self, in_f):\n",
    "        ps = in_f.split('/')\n",
    "        return ps[len(ps)-1]\n",
    "    \n",
    "\n",
    "    \n",
    "    def process(self):\n",
    "        print('* Load Drains', self.filename(self.import_file_name))\n",
    "        self.getSummary()[self.get_class_key()]={}\n",
    "        self.getSummary()[self.get_class_key()]['name']= self.filename(self.import_file_name) \n",
    "        self.getSummary()[self.get_class_key()]['before']=0\n",
    "\n",
    "        '''\n",
    "        import_file_name is the full path and name of import file\n",
    "        returns the original raw data as pandas dataframe\n",
    "        '''\n",
    "       \n",
    "            \n",
    "        self.dataframe = pd.read_csv(self.import_file_name)\n",
    "        \n",
    "        #if 'load' not in self.getSummary():\n",
    "        #    self.getSummary()['load']=[]\n",
    "            \n",
    "               \n",
    "        self.getSummary()[self.get_class_key()]['after']= len(self.dataframe)\n",
    "        #self.getSummary()['load'].append({'name': self.filename(self.import_file_name), 'records': len(self.dataframe)})\n",
    "        diff = self.getSummary()[self.get_class_key()]['after']  - self.getSummary()[self.get_class_key()]['before']\n",
    "        self.getSummary()[self.get_class_key()]['diff'] = diff\n",
    "\n",
    "class LoadDataWorld(Load):\n",
    "    '''\n",
    "    creates a dataframe with a fresh copy of the data.world dataset \n",
    "    dont forget to run\n",
    "    '''\n",
    "    def __init__(self, import_file_name):\n",
    "        Load.__init__(self,import_file_name)\n",
    "        self.summary_key='01'\n",
    "    \n",
    "    def get_class_key(self):\n",
    "        return '{}.{}'.format(self.summary_key, self.getClassName())\n",
    "    #def get_dataframe(self):\n",
    "    #    return self.dataframe\n",
    "    \n",
    "    def addColumns(self):\n",
    "        if 'dr_discharge' not in self.dataframe.columns.values:\n",
    "            self.dataframe['dr_discharge'] = self.dataframe['dr_subwatershed']\n",
    "        \n",
    "    def process(self):\n",
    "        print('* Load Data.World')\n",
    "        self.getSummary()[self.get_class_key()]={}\n",
    "        self.getSummary()[self.get_class_key()]['name']= self.filename(self.import_file_name) \n",
    "        self.getSummary()[self.get_class_key()]['before']=0\n",
    "        '''\n",
    "        import_file_name is the full path and name of import file\n",
    "        returns the original raw data as pandas dataframe\n",
    "        '''\n",
    "        # download to ~/.dw/cache/{}/latest/data/grb_drains.csv\n",
    "        self.dataframe = dw.load_dataset(self.import_file_name, auto_update=True)\n",
    "        fstr = '~/.dw/cache/{}/latest/data/grb_drains.csv'.format('citizenlabs/grb-storm-drains-2019-04-03')\n",
    "        # \n",
    "        self.dataframe = pd.read_csv(fstr)\n",
    "        \n",
    "        self.addColumns()\n",
    "        \n",
    "        for col in self.get_dataframe().columns.values:\n",
    "            print( ' -- column:', col)\n",
    "            \n",
    "        # SUMMARIZE\n",
    "        self.getSummary()[self.get_class_key()]['after']= len(self.dataframe)\n",
    "        diff = self.getSummary()[self.get_class_key()]['after']  - self.getSummary()[self.get_class_key()]['before']\n",
    "        self.getSummary()[self.get_class_key()]['diff'] = diff\n",
    "\n",
    "        #if 'load' not in self.getSummary():\n",
    "        #    self.getSummary()['load']=[]\n",
    "\n",
    "        #self.getSummary()['load'].append({'name': self.filename(self.import_file_name), 'start': len(self.dataframe)})\n",
    "        \n",
    "#LoadDataWorld('xxx.xxx').getLogger().log('hi') \n",
    "        \n",
    "#test_import_file_name = '/Users/jameswilfong/Documents/Github/Wilfongjt/01-AAD-data-world/01-In-Progress/00-03-load-spring-lake/data.world/raw-data/adopt-a-drain/CatchBasins_7_17_2019.xls.csv'\n",
    "# assert Load(testfile).get_app_name() == 'adopt-a-drain'\n",
    "#print(Load().get_repo_folder())\n",
    "#print(Load().get_raw_data_folder())\n",
    "#print(helper.get_raw_files('csv'))\n",
    "# assert Load().get_repo_folder() == 'adopt-a-drain'\n",
    "#\n",
    "#df=LoadCSV(test_import_file_name).run().get_dataframe()\n",
    "#df.info\n",
    "#df.head\n",
    "\n",
    "\n",
    "#df=LoadDW('citizenlabs/grb-storm-drains-2019-04-03').run().get_dataframe()\n",
    "\n",
    "#print(df)\n",
    "#df.info\n",
    "#df.head\n",
    "\n",
    "\n",
    "          \n",
    "process= Process()\n",
    "load = Load('afilename')\n",
    "loadDrains = LoadDrains('afilename')\n",
    "loadDataWorld = LoadDataWorld('afilename')\n",
    "assert loadDataWorld.import_file_name == 'afilename'\n",
    "assert loadDataWorld.getSummary() == {} \n",
    "assert loadDrains.import_file_name == 'afilename'\n",
    "assert loadDrains.getSummary() == {}  \n",
    "assert load.import_file_name == 'afilename'\n",
    "assert load.getSummary() == {}  \n",
    "assert process.getSummary() == {} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchDW20190925(LoadDataWorld):\n",
    "    '''\n",
    "    Delete records with duplicate coordinates from data.world dataset\n",
    "    * These records were inserted from file CatchBasins_7_17_2019.xls\n",
    "    * This patch was run only once on 09-25-2019\n",
    "    * This patch affects the dataworld file citizenlabs/grb-storm-drains-2019-04-03\n",
    "    * The CatchBasins_7_17_2019.xls were replaced by AAD_Grandville_fruitport_SpringLake_OCRC9162019\n",
    "    * A record of this patch can be found in Patch20190925.log\n",
    "    * Patch will not run if PatchDW20190925.log is found in logs/Patch20190925.log \n",
    "    * replace with LoadDataWord once everything passes\n",
    "    '''    \n",
    "    def __init__(self, import_file_name):\n",
    "        LoadDataWorld.__init__(self, import_file_name)\n",
    "        self.summary_key='01'\n",
    "    \n",
    "    def get_class_key(self):\n",
    "        return '{}.{}'.format(self.summary_key, self.getClassName())\n",
    "\n",
    "    def process(self):\n",
    "        super().process()\n",
    "        self.getLogger().kill()\n",
    "        # skip if already run\n",
    "        if path.exists('./logs/{}.log'.format(self.getClassName())):\n",
    "            print('Already ran {}...skipping'.format(self.getClassName()))\n",
    "            return \n",
    "    \n",
    "        self.getSummary()[self.get_class_key()]={}\n",
    "        self.getSummary()[self.get_class_key()]['name']= self.filename(self.import_file_name) \n",
    "        self.getSummary()[self.get_class_key()]['before']=len(self.get_dataframe())\n",
    "        \n",
    "        data = self.get_dataframe().set_index(\"dr_jurisdiction\")\n",
    "        \n",
    "        data = data.drop(\"Ottawa County Road Commission\", axis=0) # Delete all rows\n",
    "        \n",
    "        data = data.drop(\"Village of Spring Lake DPW\", axis=0) # Delete all rows \n",
    "        \n",
    "        data = data.drop(\"City of Grandville\", axis=0) # Delete all rows \n",
    "        \n",
    "        self.set_dataframe(data)\n",
    "        #print(self.get_dataframe())\n",
    "        self.getSummary()[self.get_class_key()]['after']=len(self.get_dataframe())\n",
    "        diff = self.getSummary()[self.get_class_key()]['after']  - self.getSummary()[self.get_class_key()]['before']\n",
    "        self.getSummary()[self.get_class_key()]['diff'] = diff\n",
    "        self.getLogger().log('Run')\n",
    "        \n",
    "#patch = PatchDW20190925('ssssss.ss').run()\n",
    "#patch.getLogger().log('some')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Clean(Process):\n",
    "    def __init__(self, df_source, commonNameMap, region_map):\n",
    "        Process.__init__(self)\n",
    "        self.dataframe = df_source\n",
    "        self.commonNameMap = commonNameMap\n",
    "        self.region_map=region_map\n",
    "        \n",
    "class CleanDrains(Clean):\n",
    "    def __init__(self, df_source, commonNameMap, region_map):\n",
    "        Clean.__init__(self,df_source, commonNameMap, region_map)\n",
    "        self.summary_key='03'\n",
    "    \n",
    "    def get_class_key(self):\n",
    "        return '{}.{}'.format(self.summary_key, self.getClassName())\n",
    "    \n",
    "    def clean_column_names(self):\n",
    "        '''\n",
    "        convert each column to lowercase with underscore seperation\n",
    "\n",
    "        e.g., ID to id\n",
    "        e.g., County ID to county_id\n",
    "        e.g., County-ID to county_id\n",
    "        :param actual_col_list: list of column names\n",
    "        :return: clean list of column names\n",
    "\n",
    "        {\n",
    "          'field-name': {}\n",
    "        }\n",
    "\n",
    "        '''\n",
    "        # start_time = time.time()\n",
    "        \n",
    "        actual_col_list = self.dataframe.columns\n",
    "        clean_column_names = {}\n",
    "        for cn in actual_col_list:\n",
    "        \n",
    "            ncn = cn\n",
    "            # get rid of some unwanted characters\n",
    "\n",
    "            if ' ' in cn:\n",
    "                ncn = cn.replace(' ','_')\n",
    "\n",
    "            if '-' in cn:\n",
    "                ncn = cn.replace('-', '_')\n",
    "\n",
    "            # force first char to lower case\n",
    "            nncn = ncn\n",
    "            ncn = ''\n",
    "            prev_upper = True #False\n",
    "            case = False\n",
    "            camelcase = False\n",
    "            for c in nncn:\n",
    "                if c in 'ABCDEFGHIJKLMNOPQRSTUVWXYZ':\n",
    "                    case = True\n",
    "                    if prev_upper:\n",
    "                        ncn += c.lower()\n",
    "                    else:\n",
    "                        ncn += '_' + c.lower()\n",
    "                        camelcase = True\n",
    "                    prev_upper = True\n",
    "                else:\n",
    "                    ncn += c\n",
    "                    prev_upper = False\n",
    "\n",
    "            clean_column_names[cn]=ncn\n",
    "\n",
    "        return self.dataframe.rename(columns=clean_column_names)\n",
    "        # print('* clean_column_names: {} sec'.format(time.time() - start_time))  # time_taken is in seconds\n",
    "    \n",
    "    \n",
    "    def inferName(self, col_name):\n",
    "        '''\n",
    "        select a column name based on previous names found in file\n",
    "        '''\n",
    "        names = { \n",
    "            \"subtype\": \"dr_subtype\",\n",
    "            \"jurisdicti\": \"dr_jurisdiction\",\n",
    "            \"drain__owner\": \"dr_owner\",\n",
    "            \"owner\":\"dr_owner\",\n",
    "            \"local__id\": \"dr_local_id\",\n",
    "            \"facilityid\": \"dr_facility_id\",\n",
    "            \"drain__jurisdiction\": \"dr_jurisdiction\",\n",
    "            \"subwatershed\": \"dr_subwatershed\",\n",
    "            \"subbasin\": \"dr_subwatershed\",\n",
    "            \"point__x\":\"dr_lon\", \n",
    "            \"long\": \"dr_lon\",\n",
    "            \"point__y\":\"dr_lat\",\n",
    "            \"lat\":\"dr_lat\",\n",
    "            \"soure__id\": \"del_source_id\"}\n",
    "\n",
    "        if not col_name in self.commonNameMap:\n",
    "            # mark madeup names for easy id later\n",
    "            return 'del_{}'.format(col_name)\n",
    "        \n",
    "        return self.commonNameMap[col_name]\n",
    "\n",
    "    def getColumnDict(self):\n",
    "\n",
    "        col_dict = {}\n",
    "        for nm in self.dataframe.columns.values:\n",
    "            col_dict[nm]=self.inferName(nm)  \n",
    "        return col_dict\n",
    "    \n",
    "    def remove_char(self,columnList):\n",
    "        '''\n",
    "        some facillity ids have characters mixed wtih number\n",
    "        we need just the number part\n",
    "        this function removes all characters from the facility id\n",
    "        '''\n",
    "        newList = []\n",
    "\n",
    "        for item in columnList:\n",
    "            fi = ''\n",
    "            for ch in str(item):\n",
    "                if ch in '0123456789':\n",
    "                    fi += ch\n",
    "                else:\n",
    "                    fi += '0'\n",
    "            newList.append(fi)\n",
    "\n",
    "        return newList\n",
    "        \n",
    "    def regional_codes(self, df_source , _owner):\n",
    "        '''\n",
    "        regional codes identify the data's source community\n",
    "        code are added over time. this method checks and throws error not found.\n",
    "        fix by adding new owner and code to list below\n",
    "        '''\n",
    "        #print('regional_code 1')\n",
    "        rc = []\n",
    "\n",
    "        # look at data in in the _owner column\n",
    "        for jur in self.dataframe[_owner]:\n",
    "            # check if jur is in the codes\n",
    "            if jur in self.region_map:\n",
    "                rc.append(self.region_map[jur])\n",
    "            else:\n",
    "                print('bad name', )\n",
    "                raise Exception('Regional-Code for ({}) is not available... add new '.format(jur)) \n",
    "                #rc.append('XXX')\n",
    "\n",
    "        return rc    \n",
    "    \n",
    "    def get_dataframe(self):\n",
    "        return self.dataframe\n",
    "    \n",
    "    def process(self):\n",
    "        self.getSummary()[self.get_class_key()]={}\n",
    "        self.getSummary()[self.get_class_key()]['before']=len(self.get_dataframe())\n",
    "        '''\n",
    "        clean up the df_source\n",
    "        '''\n",
    "        self.dataframe = self.clean_column_names()\n",
    "        self.dataframe = self.dataframe.rename(columns=self.getColumnDict())\n",
    "        \n",
    "        # create col to compare duplicate coordinates\n",
    "        print(' -- add coordinate')\n",
    "        self.dataframe['dup_coordinate'] = '(' + self.dataframe['dr_lon'].astype(str) + ' ' + self.dataframe['dr_lat'].astype(str) + ')'\n",
    "        #print('dup_coordinate',self.dataframe['dup_coordinate'])\n",
    "        # patch up bad owner and jurisdiction names\n",
    "        print(' -- replace dr_jurisdiction with dr_owner')\n",
    "        self.dataframe['dr_jurisdiction'] = self.dataframe['dr_owner'] # is what it is\n",
    "        \n",
    "        # mark all empties with nan\n",
    "        print(' -- identify empty dr_facility_ids')\n",
    "        self.dataframe['dr_facility_id'] = self.dataframe['dr_facility_id'].apply(lambda x:  np.nan if x != x or x == '' or x == ' ' or x == None else x)\n",
    "        \n",
    "        # some dr_facilities have alfa numeric values ... clean up\n",
    "        print(' -- remove char from dr_facility_id')\n",
    "        self.dataframe['dr_facility_id'] = self.remove_char(self.dataframe['dr_facility_id'])\n",
    "        \n",
    "        # add colunm to id the source of data records\n",
    "        print(' -- create field: source_code from dr_owner')\n",
    "        self.dataframe['source_code'] = self.regional_codes( self.dataframe , 'dr_owner')\n",
    "        \n",
    "        # convert typ to integer\n",
    "        print(' -- int-ify field: dr_facility_id')\n",
    "        self.dataframe['dr_facility_id'] = self.dataframe['dr_facility_id'].astype('int64')\n",
    "        \n",
    "        # create final id aka dr_asset_id\n",
    "        print(' -- create field: dr_asset_id from source_code, dr_facility_id')\n",
    "        self.dataframe['dr_asset_id'] = self.dataframe['source_code'] + '_'+ self.dataframe['dr_facility_id'].astype(str)\n",
    "        \n",
    "        print(' -- create field: dr_type from constant \"Storm Water Inlet Drain\"')\n",
    "        self.dataframe['dr_type'] = self.dataframe['dr_asset_id'].apply(lambda x: 'Storm Water Inlet Drain')\n",
    "        \n",
    "        # add dr_discharge column\n",
    "        print(' -- create field: dr_discharge from dr_subwatershed')\n",
    "        self.dataframe['dr_discharge'] = self.dataframe['dr_subwatershed']\n",
    "        #self.dataframe['dr_discharge'].apply(lambda x: 'none' if x != x or x == '' or x == ' ' or x == None else x)\n",
    "        \n",
    "        #print('CleanDrain', self.get_dataframe()['dr_discharge'])\n",
    "\n",
    "\n",
    "        #print(self.dataframe['dr_discharge'])\n",
    "        \n",
    "        self.getSummary()[self.get_class_key()]['after']=len(self.get_dataframe())\n",
    "        diff = self.getSummary()[self.get_class_key()]['after']  - self.getSummary()[self.get_class_key()]['before']\n",
    "        self.getSummary()[self.get_class_key()]['diff'] = diff\n",
    "        \n",
    "clean = Clean(None, maps['commonNameMap'], maps['region_map'])\n",
    "cleanDrains = CleanDrains(None, maps['commonNameMap'], maps['region_map'])\n",
    "assert cleanDrains.getSummary()=={}\n",
    "assert clean.getSummary()=={}\n",
    "\n",
    "#CleanDrains(self.get_dataframe(), self.maps['commonNameMap'], self.maps['region_map'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Outputs(Process):\n",
    "    def __init__(self, df_source, expected_output_columns_list):\n",
    "        Process.__init__(self)\n",
    "        self.dataframe = df_source\n",
    "        self.expected_output_columns_list=expected_output_columns_list\n",
    "        \n",
    "\n",
    "class OutputDrains(Outputs):\n",
    "    def __init__(self, df_source, expected_output_columns_list):\n",
    "        Outputs.__init__(self, df_source, expected_output_columns_list)\n",
    "        self.summary_key='05'\n",
    "    \n",
    "    def get_class_key(self):\n",
    "        return '{}.{}'.format(self.summary_key, self.getClassName())\n",
    "    \n",
    "    def get_dataframe(self):\n",
    "        return self.dataframe\n",
    "    \n",
    "    def set_dataframe(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "    \n",
    "    def validateOutputColumns(self):\n",
    "        # examine for extra cols not needed for data.world\n",
    "        for nm in self.get_dataframe().columns.values:\n",
    "            if not nm in self.expected_output_columns_list:   \n",
    "                raise Exception('{} is unexpected output for clean data'.format(nm))\n",
    "\n",
    "\n",
    "    def toCSV(self):    \n",
    "        self.get_dataframe().to_csv(local_config[\"local_clean\"], index=False)\n",
    "\n",
    "    def process(self):\n",
    "        print('* Outputs')\n",
    "        self.validateOutputColumns()\n",
    "        self.toCSV() # write data to disk\n",
    "        \n",
    "        # print(' - output folder: ','data.world/clean-data/adopt-a-drain/')\n",
    "        print(' - output file: ','/data.world/clean-data/adopt-a-drain/', metadata['output_file_name'] )\n",
    "        print(' - data.world file: ','/data.world/clean-data/adopt-a-drain/', metadata['copy_file_name'] )\n",
    "        \n",
    "        for colname in self.get_dataframe().columns.values:\n",
    "            print(' -- column: ', colname )\n",
    "        \n",
    "        # OUTPUT_FILE_NAME\n",
    "        \n",
    "        ifn = '{}/{}'.format(helper.get_clean_data_folder(), metadata['output_file_name'])\n",
    "        ofn = '{}/{}'.format(helper.get_clean_data_folder(), metadata['copy_file_name'])\n",
    "\n",
    "        copyfile(ifn, ofn)\n",
    "        # set up a smaller version of file\n",
    "        tfn = '{}/{}'.format(helper.get_test_version_folder(), metadata['copy_file_name'])\n",
    "        #df_small = df_source.query(\"dr_jurisdiction = 'City of Grand Rapids'\")\n",
    "        print(' - make short file for testing: ', '/data.world/test-data/adopt-a-drain/', metadata['copy_file_name'])\n",
    "        df_small=self.get_dataframe().query(\"dr_jurisdiction == 'City of Grand Rapids'\").head(5000)\n",
    "        df_small.to_csv( tfn, index=False)\n",
    "        \n",
    "outputs = Outputs(None,expected_output_columns_list)\n",
    "outputDrains = OutputDrains(None,expected_output_columns_list)\n",
    "assert outputDrains.getSummary()=={}\n",
    "assert outputs.getSummary()=={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Condense(Process):\n",
    "    def __init__(self, dataframe, expected_output_columns_list, extraColumns, outlier_settings):\n",
    "        Process.__init__(self)\n",
    "        self.dataframe = dataframe\n",
    "        self.expected_output_columns_list=expected_output_columns_list\n",
    "        self.extraColumns = extraColumns\n",
    "        self.outlier_settings = outlier_settings\n",
    "        self.summary_key='06'\n",
    "    \n",
    "    def get_class_key(self):\n",
    "        return '{}.{}'.format(self.summary_key, self.getClassName())\n",
    "        \n",
    "    def validateColumns(self):  \n",
    "        print(' - validate columns')\n",
    "        '''\n",
    "        check 's column name for the expected colnames\n",
    "        '''\n",
    "        for nm in self.expected_output_columns_list:\n",
    "            if not nm in self.get_dataframe().columns.values:\n",
    "                raise Exception('{} is missing from '.format(get_dataframe().format(nm)) )\n",
    "                \n",
    "    def removeExtraColumns(self):\n",
    "        print(' - Remove extra columns')\n",
    "        for colname in self.extraColumns:\n",
    "            if( colname in self.get_dataframe().columns.values):\n",
    "                print(' -- drop column: ',colname)\n",
    "                self.set_dataframe(self.get_dataframe().drop([colname], axis=1))\n",
    "                \n",
    "    def remove_obvious_outliers(self):\n",
    "        print(' - remove outliers')\n",
    "        '''\n",
    "        remove individual observations\n",
    "        remove range of observation\n",
    "\n",
    "        '''\n",
    "        \n",
    "        for outlier in self.outlier_settings['outliers']:\n",
    "            # pprint(outlier)\n",
    "            col_name = outlier['column']\n",
    "\n",
    "            if 'range' in outlier:\n",
    "\n",
    "                low = outlier['range'][0]\n",
    "                high = outlier['range'][1]\n",
    "                sz = len(self.get_dataframe())\n",
    "\n",
    "                tmp = None\n",
    "                tmp1 = ''\n",
    "\n",
    "                if isinstance(low, np.datetime64):\n",
    "                    self.set_dataframe(\n",
    "                      self.get_dataframe()[(self.get_dataframe()[col_name].to_datetime() >= low) & (self.get_dataframe()[col_name].to_datetime() <= high)]\n",
    "                    )\n",
    "                else:   \n",
    "                    self.set_dataframe(\n",
    "                        self.get_dataframe()[(self.get_dataframe()[col_name] >= low) & (self.get_dataframe()[col_name] <= high)]\n",
    "                    )\n",
    "                outlier[\"count\"] = sz - len(self.get_dataframe())\n",
    "\n",
    "            elif 'categories' in outlier:\n",
    "                _list = outlier['categories']\n",
    "                sz = len(self.get_dataframe())\n",
    "                self.set_dataframe(\n",
    "                    self.get_dataframe()[self.get_dataframe()[col_name].isin(_list)]\n",
    "                )\n",
    "                outlier[\"count\"] = sz - len(self.get_dataframe())\n",
    "            if \"reason\" in outlier:\n",
    "                outlier[\"reason\"] = outlier[\"reason\"].format(  str(outlier[\"count\"]) )\n",
    "\n",
    "    def remove_duplicate_assets(self):\n",
    "        print(' - drop duplicate assets')\n",
    "        \n",
    "        sz1 = len(self.get_dataframe())\n",
    "        self.set_dataframe(self.get_dataframe().drop_duplicates('dr_asset_id',keep='first'))\n",
    "        sz2 = len(self.get_dataframe())\n",
    "        \n",
    "        if 'duplicates' not in self.getSummary()[self.get_class_key()]:\n",
    "            self.getSummary()[self.get_class_key()]['duplicates']=[]\n",
    "         \n",
    "        self.getSummary()[self.get_class_key()]['duplicates'].append({'dr_asset_id': {'before':sz1, 'after':sz2, 'diff':(sz2-sz1)}})\n",
    "        #self.summary['duplicates'].append({'dr_asset_id': {'before':sz1, 'after':sz2, 'diff':(sz1-sz2)}})\n",
    "\n",
    "        # self.summary['duplicates'].append({'dr_asset_id':(sz1-sz2)})\n",
    "\n",
    "    def remove_duplicate_coordinates(self):\n",
    "        print(' - drop duplicate coordinates')\n",
    "        print(' - cols ', self.get_dataframe().columns.values)\n",
    "        \n",
    "        #print(' -- add coordinate')\n",
    "        #self.get_dataframe()['dup_coordinate'] = '(' + self.dataframe['dr_lon'].astype(str) + ' ' + self.dataframe['dr_lat'].astype(str) + ')'\n",
    "        \n",
    "        sz1 = len(self.get_dataframe())\n",
    "        \n",
    "        self.set_dataframe(self.get_dataframe().sort_values('dup_coordinate'))\n",
    "        self.set_dataframe(self.get_dataframe().drop_duplicates('dup_coordinate',keep='first'))\n",
    "        \n",
    "        sz2 = len(self.get_dataframe())\n",
    "        \n",
    "        if 'duplicates' not in self.getSummary()[self.get_class_key()]:\n",
    "            self.getSummary()[self.get_class_key()]['duplicates']=[]\n",
    "                \n",
    "        self.getSummary()[self.get_class_key()]['duplicates'].append({'coordinates': {'before':sz1, 'after':sz2, 'diff':(sz2-sz1)}})\n",
    "\n",
    "        #self.summary['duplicates'].append({'coordinates': {'before':sz1, 'after':sz2, 'diff':(sz1-sz2)}})\n",
    "        #self.summary['duplicates'].append({'coordinates':(sz1-sz2)})\n",
    "\n",
    "    def get_dataframe(self):\n",
    "        return self.dataframe\n",
    "    \n",
    "    def set_dataframe(self, dataframe):\n",
    "        self.dataframe = dataframe  \n",
    "    \n",
    "    def process(self):\n",
    "        print('* Condense')\n",
    "        self.getSummary()[self.get_class_key()]={}\n",
    "        self.getSummary()[self.get_class_key()]['before']=len(self.get_dataframe())\n",
    "        self.remove_duplicate_assets()\n",
    "        # self.remove_duplicate_coordinates()\n",
    "        self.remove_obvious_outliers()\n",
    "        \n",
    "        self.removeExtraColumns()\n",
    "        \n",
    "        self.validateColumns()\n",
    "        self.getSummary()[self.get_class_key()]['after']=len(self.get_dataframe())\n",
    "        diff=(self.getSummary()[self.get_class_key()]['before']-self.getSummary()[self.get_class_key()]['after'])\n",
    "        self.getSummary()[self.get_class_key()]['diff'] = diff\n",
    "        \n",
    "        \n",
    "class RowCondense(Condense):\n",
    "    def __init__(self, dataframe, expected_output_columns_list, extraColumns, outlier_settings):\n",
    "        Condense.__init__(self, dataframe, expected_output_columns_list, extraColumns, outlier_settings)\n",
    "        self.summary_key='04'\n",
    "    \n",
    "    def get_class_key(self):\n",
    "        return '{}.{}'.format(self.summary_key, self.getClassName())\n",
    "    \n",
    "    def process(self):\n",
    "        print('* RowCondense')\n",
    "        self.getSummary()[self.get_class_key()]={}\n",
    "        self.getSummary()[self.get_class_key()]['before']=len(self.get_dataframe())\n",
    "\n",
    "        self.remove_duplicate_assets()\n",
    "        self.remove_duplicate_coordinates()\n",
    "        self.remove_obvious_outliers()    \n",
    "\n",
    "        self.getSummary()[self.get_class_key()]['after']=len(self.get_dataframe())\n",
    "        \n",
    "        diff=(self.getSummary()[self.get_class_key()]['after']-self.getSummary()[self.get_class_key()]['before'])\n",
    "        self.getSummary()[self.get_class_key()]['diff'] = diff\n",
    "        \n",
    "\n",
    "class ColCondense(Condense):\n",
    "    def __init__(self, dataframe, expected_output_columns_list, extraColumns, outlier_settings):\n",
    "        Condense.__init__(self, dataframe, expected_output_columns_list, extraColumns, outlier_settings)\n",
    "        self.summary_key='05'\n",
    "    \n",
    "    def get_class_key(self):\n",
    "        return '{}.{}'.format(self.summary_key, self.getClassName())\n",
    "    \n",
    "    def process(self):\n",
    "        print('* ColCondense')\n",
    "        self.getSummary()[self.get_class_key()]={}\n",
    "        self.getSummary()[self.get_class_key()]['from']=[c for c in self.get_dataframe().columns]\n",
    "        \n",
    "        self.removeExtraColumns()\n",
    "        \n",
    "        self.validateColumns()\n",
    "        self.getSummary()[self.get_class_key()]['to']=[c for c in self.get_dataframe().columns]\n",
    "        #diff=(self.getSummary()[self.get_class_key()]['before']-self.getSummary()[self.get_class_key()]['after'])\n",
    "        #self.getSummary()[self.get_class_key()]['diff'] = diff\n",
    "        \n",
    "                \n",
    "        \n",
    "condense=Condense(None, expected_output_columns_list, extraColumns, outlier_settings)\n",
    "assert loadDataWorld.import_file_name == 'afilename'\n",
    "assert loadDataWorld.getSummary() == {}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wrangle(Process):\n",
    "    def __init__(self, maps, expected_output_columns_list):\n",
    "        Process.__init__(self)\n",
    "        self.maps=maps\n",
    "        self.expected_output_columns_list=expected_output_columns_list\n",
    "        self.summary_key='00'\n",
    "    \n",
    "    def get_class_key(self):\n",
    "        return '{}.{}'.format(self.summary_key, self.getClassName())\n",
    "    \n",
    "    def get_dataframe(self):\n",
    "        return self.dataframe\n",
    "    \n",
    "    def set_dataframe(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "    \n",
    "    def xls2csv(self,xls_name):\n",
    "        import xlrd\n",
    "        # generates a csv file from the first sheet in an excel file\n",
    "\n",
    "        wb = xlrd.open_workbook(xls_name)\n",
    "        sh = wb.sheet_by_index(0)\n",
    "        your_csv_file = open('{}.csv'.format(xls_name), 'w', encoding='utf8')\n",
    "\n",
    "        wr = csv.writer(your_csv_file, quoting=csv.QUOTE_ALL)\n",
    "        for rownum in range(sh.nrows):\n",
    "            wr.writerow(sh.row_values(rownum))\n",
    "\n",
    "        your_csv_file.close()\n",
    "\n",
    "\n",
    "    def filename(self, in_f):\n",
    "        ps = in_f.split('/')\n",
    "        return ps[len(ps)-1]\n",
    "        \n",
    "    def conversions(self):\n",
    "        for xls in helper.get_raw_files('xls'):\n",
    "            print(' -- convert ', xls)\n",
    "            self.xls2csv(xls)\n",
    "        for xlsx in helper.get_raw_files('xlsx'):\n",
    "            print(' -- convert ', xlsx)\n",
    "            self.xls2csv(xlsx)    \n",
    "    '''        \n",
    "    def finalCSV(self):\n",
    "        \n",
    "        self.get_dataframe().to_csv(local_config[\"local_clean\"], index=False)\n",
    "    '''    \n",
    "    def process(self):\n",
    "        print('* Wrangle')\n",
    "        \n",
    "        print('self.get_class_key()', self.get_class_key())\n",
    "        #self.getSummary()[self.get_class_key()]={}\n",
    "        #self.getSummary()[self.get_class_key()]['before']=0\n",
    "        \n",
    "        # get list of raw data files\n",
    "        print(' - raw folder ', helper.get_raw_data_folder())\n",
    "        # print(helper.get_raw_files('csv'))\n",
    "        raw_folder = helper.get_raw_data_folder()\n",
    "        clean_folder = helper.get_clean_data_folder()\n",
    "\n",
    "        concat_list = []\n",
    "        #* load data\n",
    "        #* convert xls to csv\n",
    "        #* fix column names\n",
    "        #* map expected colums to raw-data columns\n",
    "        #* drop drains without a facility id\n",
    "        #* fix column types\n",
    "\n",
    "        self.conversions() # convert excel files to csv\n",
    "        print('####### LOAD DW 01')\n",
    "        # load these up first\n",
    "\n",
    "        loadDataWorld = PatchDW20190925(dw_source)\n",
    "\n",
    "        loadDataWorld\\\n",
    "            .setSummary(self.getSummary())\\\n",
    "            .run() # a\n",
    "        concat_list.append( loadDataWorld.get_dataframe() )     \n",
    "        \n",
    "        # load up the files in the raw data folder\n",
    "        for in_f in helper.get_raw_files('csv'):\n",
    "            # print(' - raw: ', self.filename(in_f))\n",
    "            print('####### LOAD DRAINS 02')\n",
    "\n",
    "            # LOAD\n",
    "            loadDrains = LoadDrains(in_f)\\\n",
    "                .setSummary(self.getSummary())\\\n",
    "                .run() # b\n",
    "                    \n",
    "            self.set_dataframe( loadDrains.get_dataframe())\n",
    "            \n",
    "             # CLEAN\n",
    "            print('####### CLEAN DRAINS 03')\n",
    "            cleanDrains=CleanDrains(self.get_dataframe(), \n",
    "                                            self.maps['commonNameMap'], \n",
    "                                            self.maps['region_map'])\\\n",
    "                .setSummary(self.getSummary())\\\n",
    "                .run() # c\n",
    "                \n",
    "            self.set_dataframe( cleanDrains.get_dataframe() )\n",
    "            \n",
    "            # CONDENCE individual datasets\n",
    "            print('####### ROWCONDENCE DRAINS 04')\n",
    "\n",
    "            rowCondense = RowCondense(self.get_dataframe(),\\\n",
    "                                     expected_output_columns_list,\\\n",
    "                                     extraColumns,\\\n",
    "                                     outlier_settings)\\\n",
    "                .setSummary(self.getSummary())\\\n",
    "                .run() # d\n",
    "                \n",
    "            self.set_dataframe(rowCondense.get_dataframe())\n",
    "            \n",
    "            print('####### COLCONDENCE DRAINS 04')\n",
    "\n",
    "            colCondense = ColCondense(self.get_dataframe(),\\\n",
    "                                     expected_output_columns_list,\\\n",
    "                                     extraColumns,\\\n",
    "                                     outlier_settings)\\\n",
    "                .setSummary(self.getSummary())\\\n",
    "                .run() # d\n",
    "                \n",
    "            self.set_dataframe(colCondense.get_dataframe())\n",
    "            \n",
    "            # COMPILE\n",
    "            print('####### COMPILE DRAINS')\n",
    "            concat_list.append(self.get_dataframe())\n",
    "                               \n",
    "        '''\n",
    "        --------------------------------- combine datasets\n",
    "        '''  \n",
    "        print('concat_list', len(concat_list))\n",
    "        for ds in concat_list:\n",
    "            print('df cnt',len(ds))\n",
    "        self.set_dataframe( pd.concat(concat_list) )\n",
    "        \n",
    "        '''\n",
    "        --------------------------------- Condense dataset (cols, rows)\n",
    "        '''\n",
    "\n",
    "        #self.getSummary()[self.get_class_key()]['before']=len(self.get_dataframe())\n",
    "        \n",
    "        print('####### CONDENSE ALL 05')\n",
    "\n",
    "\n",
    "        condense = Condense(self.get_dataframe(),\\\n",
    "                                     expected_output_columns_list,\\\n",
    "                                     extraColumns,\\\n",
    "                                     outlier_settings)\\\n",
    "            .setSummary(self.getSummary())\\\n",
    "            .run() # e\n",
    "        \n",
    "        self.set_dataframe( condense.get_dataframe() )\n",
    "\n",
    "        '''\n",
    "        --------------------------------- save csv \n",
    "        '''\n",
    "        # assume new file and remove old one\n",
    "        local_config[\"local_clean\"]='{}/{}'.format(helper.get_clean_data_folder(),metadata['output_file_name'])\n",
    "\n",
    "        if os.path.isfile(local_config[\"local_clean\"]):\n",
    "            os.remove(local_config['local_clean'])\n",
    "            cell_log.collect('* deleted {} '.format(local_config['local_clean']))\n",
    "\n",
    "        #self.getSummary()[self.get_class_key()]['after']=len(self.get_dataframe())\n",
    "        #diff = self.getSummary()[self.get_class_key()]['after']  - self.getSummary()[self.get_class_key()]['before']\n",
    "        #self.getSummary()[self.get_class_key()]['diff'] = diff\n",
    "        \n",
    "        pprint(self.getSummary())\n",
    "            \n",
    "        # stop if columns are not expected\n",
    "        #self.validateOutputColumns()\n",
    "       \n",
    "        # self.finalCSV()\n",
    "        #df_source.to_csv(local_config[\"local_clean\"], index=False)\n",
    "        OutputDrains(self.get_dataframe(), self.expected_output_columns_list).run()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrangling Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "* Wrangle\n",
      "self.get_class_key() 00.Wrangle\n",
      " - raw folder  /Users/jameswilfong/Documents/Github/Wilfongjt/01-AAD-data-world/01-In-Progress/06-20190924-data-load/data.world/raw-data/adopt-a-drain\n",
      " -- convert  /Users/jameswilfong/Documents/Github/Wilfongjt/01-AAD-data-world/01-In-Progress/06-20190924-data-load/data.world/raw-data/adopt-a-drain/AAD_Grandville_fruitport_SpringLake_OCRC9162019.xlsx\n",
      "####### LOAD DW 01\n",
      "* Load Data.World\n",
      " -- column: dr_asset_id\n",
      " -- column: dr_discharge\n",
      " -- column: dr_jurisdiction\n",
      " -- column: dr_lat\n",
      " -- column: dr_lon\n",
      " -- column: dr_owner\n",
      " -- column: dr_subtype\n",
      " -- column: dr_subwatershed\n",
      " -- column: dr_type\n",
      " -- column: dr_location\n",
      "kill log:  ./logs/PatchDW20190925.log\n",
      "####### LOAD DRAINS 02\n",
      "* Load Drains AAD_Grandville_fruitport_SpringLake_OCRC9162019.xlsx.csv\n",
      "####### CLEAN DRAINS 03\n",
      " -- add coordinate\n",
      " -- replace dr_jurisdiction with dr_owner\n",
      " -- identify empty dr_facility_ids\n",
      " -- remove char from dr_facility_id\n",
      " -- create field: source_code from dr_owner\n",
      " -- int-ify field: dr_facility_id\n",
      " -- create field: dr_asset_id from source_code, dr_facility_id\n",
      " -- create field: dr_type from constant \"Storm Water Inlet Drain\"\n",
      " -- create field: dr_discharge from dr_subwatershed\n",
      "####### ROWCONDENCE DRAINS 04\n",
      "* RowCondense\n",
      " - drop duplicate assets\n",
      " - drop duplicate coordinates\n",
      " - cols  ['del_fid' 'dr_subtype' 'dr_jurisdiction' 'dr_owner' 'del_source'\n",
      " 'dr_local_id' 'dr_facility_id' 'dr_lat' 'dr_lon' 'dr_subwatershed'\n",
      " 'dup_coordinate' 'source_code' 'dr_asset_id' 'dr_type' 'dr_discharge']\n",
      " - remove outliers\n",
      "####### COLCONDENCE DRAINS 04\n",
      "* ColCondense\n",
      " - Remove extra columns\n",
      " -- drop column:  del_source\n",
      " -- drop column:  del_fid\n",
      " -- drop column:  source_code\n",
      " -- drop column:  dr_local_id\n",
      " -- drop column:  dr_facility_id\n",
      " -- drop column:  dup_coordinate\n",
      " - validate columns\n",
      "####### COMPILE DRAINS\n",
      "concat_list 2\n",
      "df cnt 44017\n",
      "df cnt 1517\n",
      "####### CONDENSE ALL 05\n",
      "* Condense\n",
      " - drop duplicate assets\n",
      " - remove outliers\n",
      " - Remove extra columns\n",
      " -- drop column:  dr_location\n",
      " - validate columns\n",
      "{'01.PatchDW20190925': {'after': 44017,\n",
      "                        'before': 45525,\n",
      "                        'diff': -1508,\n",
      "                        'name': 'grb-storm-drains-2019-04-03'},\n",
      " '02.LoadDrains': {'after': 1594,\n",
      "                   'before': 0,\n",
      "                   'diff': 1594,\n",
      "                   'name': 'AAD_Grandville_fruitport_SpringLake_OCRC9162019.xlsx.csv'},\n",
      " '03.CleanDrains': {'after': 1594, 'before': 1594, 'diff': 0},\n",
      " '04.RowCondense': {'after': 1517,\n",
      "                    'before': 1594,\n",
      "                    'diff': -77,\n",
      "                    'duplicates': [{'dr_asset_id': {'after': 1518,\n",
      "                                                    'before': 1594,\n",
      "                                                    'diff': -76}},\n",
      "                                   {'coordinates': {'after': 1517,\n",
      "                                                    'before': 1518,\n",
      "                                                    'diff': -1}}]},\n",
      " '05.ColCondense': {'from': ['del_fid',\n",
      "                             'dr_subtype',\n",
      "                             'dr_jurisdiction',\n",
      "                             'dr_owner',\n",
      "                             'del_source',\n",
      "                             'dr_local_id',\n",
      "                             'dr_facility_id',\n",
      "                             'dr_lat',\n",
      "                             'dr_lon',\n",
      "                             'dr_subwatershed',\n",
      "                             'dup_coordinate',\n",
      "                             'source_code',\n",
      "                             'dr_asset_id',\n",
      "                             'dr_type',\n",
      "                             'dr_discharge'],\n",
      "                    'to': ['dr_subtype',\n",
      "                           'dr_jurisdiction',\n",
      "                           'dr_owner',\n",
      "                           'dr_lat',\n",
      "                           'dr_lon',\n",
      "                           'dr_subwatershed',\n",
      "                           'dr_asset_id',\n",
      "                           'dr_type',\n",
      "                           'dr_discharge']},\n",
      " '06.Condense': {'after': 45534,\n",
      "                 'before': 45534,\n",
      "                 'diff': 0,\n",
      "                 'duplicates': [{'dr_asset_id': {'after': 45534,\n",
      "                                                 'before': 45534,\n",
      "                                                 'diff': 0}}]}}\n",
      "* Outputs\n",
      " - output file:  /data.world/clean-data/adopt-a-drain/ grb_drains-2019-09-026.csv\n",
      " - data.world file:  /data.world/clean-data/adopt-a-drain/ grb_drains.csv\n",
      " -- column:  dr_asset_id\n",
      " -- column:  dr_discharge\n",
      " -- column:  dr_jurisdiction\n",
      " -- column:  dr_lat\n",
      " -- column:  dr_lon\n",
      " -- column:  dr_owner\n",
      " -- column:  dr_subtype\n",
      " -- column:  dr_subwatershed\n",
      " -- column:  dr_type\n",
      " - make short file for testing:  /data.world/test-data/adopt-a-drain/ grb_drains.csv\n"
     ]
    }
   ],
   "source": [
    "# NEW CELL\n",
    "# testing \n",
    "# current dataset from dataworld\n",
    "dw_source = 'citizenlabs/grb-storm-drains-2019-04-03'\n",
    "summary = {}\n",
    "wrangle=Wrangle(maps, expected_output_columns_list).setSummary(summary)\n",
    "pprint(wrangle.getSummary())\n",
    "if ENV_ERROR:\n",
    "    cell_log.collect(\"# Script Failure!!\")\n",
    "    cell_log.collect(\"# !!! Missing Environment Variables !!!\")\n",
    "    cell_log.collect(\"### see [Environment Variable Setup](#env-setup)\")\n",
    "else:\n",
    "    # get list of raw data files\n",
    "    wrangle.run()\n",
    "\n",
    "#print('wrangle dr_discharge', wrangle.get_dataframe()['dr_discharge'])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output new CSV File\n",
    "* replacement for data.world and the production db\n",
    "* the small version for the test db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 45534 entries, City of Wyoming to 1072\n",
      "Data columns (total 9 columns):\n",
      "dr_asset_id        45534 non-null object\n",
      "dr_discharge       45412 non-null object\n",
      "dr_jurisdiction    1517 non-null object\n",
      "dr_lat             45534 non-null float64\n",
      "dr_lon             45534 non-null float64\n",
      "dr_owner           45534 non-null object\n",
      "dr_subtype         45534 non-null float64\n",
      "dr_subwatershed    45412 non-null object\n",
      "dr_type            45534 non-null object\n",
      "dtypes: float64(3), object(6)\n",
      "memory usage: 3.5+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dr_asset_id</th>\n",
       "      <th>dr_discharge</th>\n",
       "      <th>dr_jurisdiction</th>\n",
       "      <th>dr_lat</th>\n",
       "      <th>dr_lon</th>\n",
       "      <th>dr_owner</th>\n",
       "      <th>dr_subtype</th>\n",
       "      <th>dr_subwatershed</th>\n",
       "      <th>dr_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>City of Wyoming</th>\n",
       "      <td>CWY_40089252</td>\n",
       "      <td>Buck Creek</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.892421</td>\n",
       "      <td>-85.704451</td>\n",
       "      <td>City of Wyoming</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Buck Creek</td>\n",
       "      <td>Storm Water Inlet Drain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>City of Wyoming</th>\n",
       "      <td>CWY_40089269</td>\n",
       "      <td>Buck Creek</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.898591</td>\n",
       "      <td>-85.649908</td>\n",
       "      <td>City of Wyoming</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Buck Creek</td>\n",
       "      <td>Storm Water Inlet Drain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kent County Road Commission</th>\n",
       "      <td>KCRC_40088536</td>\n",
       "      <td>Plaster Creek</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.833919</td>\n",
       "      <td>-85.615018</td>\n",
       "      <td>Kent County Road Commission</td>\n",
       "      <td>16.0</td>\n",
       "      <td>Plaster Creek</td>\n",
       "      <td>Storm Water Inlet Drain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kent County Road Commission</th>\n",
       "      <td>KCRC_40088538</td>\n",
       "      <td>Direct Drainage to Lower Grand River</td>\n",
       "      <td>NaN</td>\n",
       "      <td>43.065199</td>\n",
       "      <td>-85.636698</td>\n",
       "      <td>Kent County Road Commission</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Direct Drainage to Lower Grand River</td>\n",
       "      <td>Storm Water Inlet Drain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>City of Wyoming</th>\n",
       "      <td>CWY_40088561</td>\n",
       "      <td>Buck Creek</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.887989</td>\n",
       "      <td>-85.689530</td>\n",
       "      <td>City of Wyoming</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Buck Creek</td>\n",
       "      <td>Storm Water Inlet Drain</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               dr_asset_id  \\\n",
       "City of Wyoming               CWY_40089252   \n",
       "City of Wyoming               CWY_40089269   \n",
       "Kent County Road Commission  KCRC_40088536   \n",
       "Kent County Road Commission  KCRC_40088538   \n",
       "City of Wyoming               CWY_40088561   \n",
       "\n",
       "                                                     dr_discharge  \\\n",
       "City of Wyoming                                        Buck Creek   \n",
       "City of Wyoming                                        Buck Creek   \n",
       "Kent County Road Commission                         Plaster Creek   \n",
       "Kent County Road Commission  Direct Drainage to Lower Grand River   \n",
       "City of Wyoming                                        Buck Creek   \n",
       "\n",
       "                            dr_jurisdiction     dr_lat     dr_lon  \\\n",
       "City of Wyoming                         NaN  42.892421 -85.704451   \n",
       "City of Wyoming                         NaN  42.898591 -85.649908   \n",
       "Kent County Road Commission             NaN  42.833919 -85.615018   \n",
       "Kent County Road Commission             NaN  43.065199 -85.636698   \n",
       "City of Wyoming                         NaN  42.887989 -85.689530   \n",
       "\n",
       "                                                dr_owner  dr_subtype  \\\n",
       "City of Wyoming                          City of Wyoming        15.0   \n",
       "City of Wyoming                          City of Wyoming        15.0   \n",
       "Kent County Road Commission  Kent County Road Commission        16.0   \n",
       "Kent County Road Commission  Kent County Road Commission        15.0   \n",
       "City of Wyoming                          City of Wyoming        15.0   \n",
       "\n",
       "                                                  dr_subwatershed  \\\n",
       "City of Wyoming                                        Buck Creek   \n",
       "City of Wyoming                                        Buck Creek   \n",
       "Kent County Road Commission                         Plaster Creek   \n",
       "Kent County Road Commission  Direct Drainage to Lower Grand River   \n",
       "City of Wyoming                                        Buck Creek   \n",
       "\n",
       "                                             dr_type  \n",
       "City of Wyoming              Storm Water Inlet Drain  \n",
       "City of Wyoming              Storm Water Inlet Drain  \n",
       "Kent County Road Commission  Storm Water Inlet Drain  \n",
       "Kent County Road Commission  Storm Water Inlet Drain  \n",
       "City of Wyoming              Storm Water Inlet Drain  "
      ]
     },
     "execution_count": 733,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrangle.get_dataframe().info()\n",
    "wrangle.get_dataframe().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
