{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Adopt a Drain Data\n",
    " * Author: James Wilfong, wilfongjt@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goals\n",
    "* Merge drains from multiple communities across the watershed\n",
    "* Resolve dataset conflicts i.e., drain identifyer uniqueness, data type, column names, and value range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limits\n",
    " * Outputs from this script need to be manually pushed to the data.world repository (repo)\n",
    " * Doesn't handle deleted drains\n",
    " \n",
    "## Assumptions\n",
    "* Assume the community-dataset is a subset of all drains in the universe ;)\n",
    "* Assume record identifiers are not unique across communities \n",
    "* Assume the community-dataset's column names are not the same across communities \n",
    "* Assume the community-dataset's data types are not the same across communities\n",
    "* Assume the community-dataset's data-values ranges are not the same across communities\n",
    "* Assume the community-dataset has duplicate records\n",
    "* Assume the community-dataset's format is Comma Seperated Values (CSV) or Excel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs\n",
    " * The current live dataset is downloaded from data.world \n",
    " * Updates are put into the raw-data/adopt-a-drain of the data.world repo.\n",
    "\n",
    "## Process\n",
    "  The process is initiated by running this Jupyter Notebook.\n",
    " * load current drains dataset from data.world\n",
    " * load dataset(s) from raw-data/adopt-a-drain folder\n",
    " * clean data: create unique id from facility id etal.\n",
    " * clean data: remove characters from facility ids\n",
    " * clean data: map input columns to expected output columns\n",
    " * clean data: fix common data problems\n",
    " * condense data: remove drains with no facility id\n",
    " * condense data: remove unneeded columns\n",
    " * condense data: remove outliers\n",
    " * condense: merge dataworld and new data \n",
    " * condense data: remove duplicate drains (keep the first duplicate)\n",
    " * concat: Make one big dataset from one or many\n",
    "\n",
    "## Outputs\n",
    " * Clean data is output to the clean-data/adopt-a-drain folder.\n",
    " * save big dataset to clean-data/adopt-a-drain/grb_drains.csv\n",
    " * save big dataset to clean-data/grb_drains-2019-08-020.csv\n",
    " * \n",
    " \n",
    "## Next Steps\n",
    " * Push updates to repo\n",
    " * Sync data from github to dataworld\n",
    "     * Sync Manually\n",
    "     * or Wait for the weekly auto-sync\n",
    " * Update the Adopt a Drain database\n",
    "     * Run Ruby rake process in Heroku "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "import settings\n",
    "from IPython.display import display, HTML\n",
    "from IPython.display import Markdown\n",
    "from lib.p3_ProcessLogger import ProcessLogger\n",
    "\n",
    "import helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cell_log = ProcessLogger() \n",
    "#cell_log.clear()\n",
    "#import interface\n",
    "#cell_log.collect('* Import third party packages')\n",
    "import sys\n",
    "import time\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import csv # read and write csv files\n",
    "from pprint import pprint\n",
    "import os\n",
    "\n",
    "import datadotworld as dw\n",
    "\n",
    "# import subprocess\n",
    "\n",
    "# convenience functions -- cleaning\n",
    "# cell_log.collect('* Import custom packages')\n",
    "from lib.p3_CellCounts import CellCounts\n",
    "# import lib.p3_clean as clean\n",
    "from lib.p3_configuration import get_configuration\n",
    "import lib.p3_explore as explore\n",
    "#import lib.p3_gather as gather # gathering functions\n",
    "# import lib.p3_helper_functions as helper\n",
    "import lib.p3_map as maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTANT\n",
    "## Configuring the Data Transfer\n",
    "Configure before running \"RUN All\" in the Cell menu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kill log:  ./maintainer/maintainer-config.json\n",
      "Metadata =============\n",
      "{'copy_file_name': 'grb_drains.csv',\n",
      " 'desc': 'Storm Drains of the Grand River Basin, Michigan',\n",
      " 'gh_file_type': 'csv',\n",
      " 'output_file_name': 'grb_drains-2019-08-024.csv',\n",
      " 'table_name': 'grb_drains',\n",
      " 'title': 'GRB Storm Drains'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "'''\n",
    "    Input: CSV file in raw_data/ folder\n",
    "    Process: clean (conform, condence)\n",
    "    Output: is directed to the clean-data/ folder\n",
    "    \n",
    "    Name the output file using OUTPUT_FILE\n",
    "    OUTPUT_FILELOCAL_CLEAN_NAME is used to name the data.world table\n",
    "    Table names should start with letter, may contain letters, numbers, underscores\n",
    "    \n",
    "'''\n",
    "cell_log = ProcessLogger() \n",
    "cell_log.clear()\n",
    "table_name='grb_drains'\n",
    "\n",
    "metadata = {\n",
    "    'output_file_name': '{}-{}.csv'.format(table_name, helper.get_daystamp(),'csv'),\n",
    "    'copy_file_name': '{}.csv'.format(table_name),\n",
    "    'gh_file_type': 'csv',\n",
    "    'title': 'GRB Storm Drains',\n",
    "    'desc': 'Storm Drains of the Grand River Basin, Michigan',\n",
    "    'table_name': table_name\n",
    "}\n",
    "\n",
    "helper.exportMaintainerConfig(metadata['output_file_name'], \n",
    "                              metadata['gh_file_type'], \n",
    "                              metadata['title'], \n",
    "                              metadata['desc'], \n",
    "                              metadata['table_name'])    \n",
    "print('Metadata =============')\n",
    "pprint(metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local_config ===========\n",
      "{'app_name': 'adopt-a-drain', 'local_clean': None, 'local_raw': None}\n",
      "outlier_settings ===========\n",
      "{'outliers': [{'column': 'dr_lon',\n",
      "               'count': 0,\n",
      "               'range': (-90.0, -80.0),\n",
      "               'reason': 'Remove {} observations too far west or east.'},\n",
      "              {'column': 'dr_lat',\n",
      "               'count': 0,\n",
      "               'range': (40.0, 50.0),\n",
      "               'reason': 'Remove {} observations too far north or south.'}]}\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Assemble Names of:\n",
    "        Application,\n",
    "        Raw data file,\n",
    "        Clean data file\n",
    "'''\n",
    "\n",
    "local_config = { \n",
    "                 \"app_name\": helper.get_app_name(),\n",
    "                 \"local_raw\": None,\n",
    "                 \"local_clean\": None\n",
    "               }\n",
    "\n",
    "'''\n",
    "    ------------- configure outliers\n",
    "'''\n",
    "# _outliers = {\n",
    "outlier_settings = {\n",
    "  'outliers': [\n",
    "    {'column':'dr_lon',\n",
    "     'range':(-90.0, -80.0),\n",
    "     'reason':'Remove {} observations too far west or east.',\n",
    "     'count': 0\n",
    "    },  \n",
    "    {'column':'dr_lat',\n",
    "     'range':(40.0, 50.0),\n",
    "     'reason':'Remove {} observations too far north or south.',\n",
    "     'count': 0\n",
    "    }\n",
    "  ]\n",
    "}  \n",
    "ENV_ERROR=False\n",
    "print(\"local_config ===========\")\n",
    "pprint(local_config)\n",
    "print(\"outlier_settings ===========\")\n",
    "pprint(outlier_settings)\n",
    "#print('Local_RAW_FILE: ', LOCAL_RAW_FILE)\n",
    "#print('LOCAL_CLEAN_FILE: ',LOCAL_CLEAN_FILE )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if ENV_ERROR:\n",
    "    cell_log.collect(\"# Script Failure!!\")\n",
    "    cell_log.collect(\"# !!! Missing Environment Variables !!!\")\n",
    "    cell_log.collect(\"### see [Environment Variable Setup](#env-setup)\")\n",
    "\n",
    "Markdown('''{}'''.format(cell_log.getMarkdown()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef inferName(col_name):\\n    # select a column name based on previous names found in file\\n    \\n    names = { \\n        \"subtype\": \"dr_subtype\",\\n        \"jurisdicti\": \"dr_jurisdiction\",\\n        \"drain__owner\": \"dr_owner\",\\n        \"owner\":\"dr_owner\",\\n        \"local__id\": \"dr_local_id\",\\n        \"facilityid\": \"dr_facility_id\",\\n        \"drain__jurisdiction\": \"dr_jurisdiction\",\\n        \"subwatershed\": \"dr_subwatershed\",\\n        \"subbasin\": \"dr_subwatershed\",\\n        \"point__x\":\"dr_lon\", \\n        \"long\": \"dr_lon\",\\n        \"point__y\":\"dr_lat\",\\n        \"lat\":\"dr_lat\",\\n        \"soure__id\": \"del_source_id\"}\\n    \\n    if not col_name in names:\\n        # mark madeup names for easy id later\\n        return \\'del_{}\\'.format(col_name)\\n    return names[col_name]\\n\\nprint(\\'owner maps to {}\\'.format(inferName(\\'owner\\')))\\n'"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# common names from imported files and how then map to actual names\n",
    "maps ={\n",
    "    \"commonNameMap\": { \n",
    "        \"subtype\": \"dr_subtype\",\n",
    "        \"jurisdicti\": \"dr_jurisdiction\",\n",
    "        \"drain__owner\": \"dr_owner\",\n",
    "        \"owner\":\"dr_owner\",\n",
    "        \"local__id\": \"dr_local_id\",\n",
    "        \"facilityid\": \"dr_facility_id\",\n",
    "        \"drain__jurisdiction\": \"dr_jurisdiction\",\n",
    "        \"subwatershed\": \"dr_subwatershed\",\n",
    "        \"subbasin\": \"dr_subwatershed\",\n",
    "        \"point__x\":\"dr_lon\", \n",
    "        \"long\": \"dr_lon\",\n",
    "        \"point__y\":\"dr_lat\",\n",
    "        \"lat\":\"dr_lat\",\n",
    "        \"soure__id\": \"del_source_id\"\n",
    "    },\n",
    "\n",
    "    \"region_map\": {\n",
    "        \"Kent County Road Commission\": \"KCRC\",\n",
    "        \"KENT COUNTY ROAD COMMISSION\":\"KCRC\",\n",
    "        \"City of East Grand Rapids\": \"EGR\",\n",
    "        \"City of Grandville\": \"GRANDV\",\n",
    "        \"City of Wyoming\": \"CWY\",\n",
    "        \"City of Kentwood\": \"CK\",\n",
    "        \"Grand Rapids Township\": \"GRTWP\",\n",
    "        \"City of Walker\": \"CW\",\n",
    "        \"CGR\": \"CGR\",\n",
    "        \"City of Grand Rapids\": \"CGR\",\n",
    "        \"Georgetown Township\": \"GTWP\",\n",
    "        \"City of Hudsonville\": \"CHV\",\n",
    "        \"Jamestown Township\": \"JTTWP\",\n",
    "        \"Cascade Township\": \"CASTWP\",\n",
    "        \"Algoma Township\": \"ALGTWP\",\n",
    "        \"Grattan Township\": \"GRATWP\",\n",
    "        \"Gaines Township\": \"GAITWP\",\n",
    "        \"Vergennes Township\": \"VERTWP\",\n",
    "        \"Lowell Township\": \"LOWTWP\",\n",
    "        \"Oakfield Township\": \"OAKTWP\",\n",
    "        \"Cannon Township\": \"CANTWP\",\n",
    "        \"Sparta Township\": \"SPATWP\",\n",
    "        \"Solon Township\": \"SOLTWP\",\n",
    "        \"Ada Township\": \"ADATWP\",\n",
    "        \"City of Lowell\": \"CLO\",\n",
    "        \"Bowne Township\": \"BOWTWP\",\n",
    "        \"Tyrone Township\": \"TYRTWP\",\n",
    "        \"Caledonia Township\": \"CALTWP\",\n",
    "        \"Courtland Township\": \"COUTWP\",\n",
    "        \"Spencer Township\": \"SPETWP\",\n",
    "        \"Village of Sparta\": \"VSP\",\n",
    "        \"BYRON TOWNSHIP\": \"BYRTWP\",\n",
    "        \"CALEDONIA TOWNSHIP\": \"CALETWP\",\n",
    "        \"City of Rockford\": \"CRF\",\n",
    "        \"Alpine Township\": \"ALPTWP\",\n",
    "        \"Plainfield Township\": \"PLATWP\",\n",
    "        \"Byron Township\": \"BYRTWP\",\n",
    "        \"OCWRC\": \"OCWRC\",\n",
    "        \"City of Grand Haven DPW\":\"CGH\",\n",
    "        \"Village of Spring Lake DPW\": \"VSL\",\n",
    "        \"Ottawa County Road Commission\": \"OCRC\",\n",
    "        \"OCRC\": \"OCRC\"\n",
    "    }\n",
    "}\n",
    "extraColumns = ['del_source',\n",
    "              'del_fid',\n",
    "              'del_gid',\n",
    "              'source_code',\n",
    "              'dr_local_id',\n",
    "              'dr_facility_id',\n",
    "              'dr_location']\n",
    "# expected_columns_list = ['facility_prefix', \n",
    "expected_process_columns_list = ['facility_prefix', \n",
    "                         'dr_subtype', \n",
    "                         'dr_jurisdiction', \n",
    "                         'dr_owner', \n",
    "                         'dr_subwatershed', \n",
    "                         'dr_lon', \n",
    "                         'dr_lat', \n",
    "                         'dr_asset_id', \n",
    "                         'dr_type', \n",
    "                         'dr_sync_id']\n",
    "\n",
    "\n",
    "expected_output_columns_list=['dr_subtype',\n",
    "                              'dr_jurisdiction',\n",
    "                              'dr_owner',\n",
    "                              'dr_lat',\n",
    "                              'dr_lon',\n",
    "                              'dr_subwatershed',\n",
    "                              'dr_asset_id', \n",
    "                              'dr_type']\n",
    "\n",
    "\n",
    "'''\n",
    "def inferName(col_name):\n",
    "    # select a column name based on previous names found in file\n",
    "    \n",
    "    names = { \n",
    "        \"subtype\": \"dr_subtype\",\n",
    "        \"jurisdicti\": \"dr_jurisdiction\",\n",
    "        \"drain__owner\": \"dr_owner\",\n",
    "        \"owner\":\"dr_owner\",\n",
    "        \"local__id\": \"dr_local_id\",\n",
    "        \"facilityid\": \"dr_facility_id\",\n",
    "        \"drain__jurisdiction\": \"dr_jurisdiction\",\n",
    "        \"subwatershed\": \"dr_subwatershed\",\n",
    "        \"subbasin\": \"dr_subwatershed\",\n",
    "        \"point__x\":\"dr_lon\", \n",
    "        \"long\": \"dr_lon\",\n",
    "        \"point__y\":\"dr_lat\",\n",
    "        \"lat\":\"dr_lat\",\n",
    "        \"soure__id\": \"del_source_id\"}\n",
    "    \n",
    "    if not col_name in names:\n",
    "        # mark madeup names for easy id later\n",
    "        return 'del_{}'.format(col_name)\n",
    "    return names[col_name]\n",
    "\n",
    "print('owner maps to {}'.format(inferName('owner')))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Process():\n",
    "    \n",
    "    def getClassName(self):\n",
    "        return self.__class__.__name__\n",
    "    \n",
    "    def process(self):\n",
    "        raise Exception('Overload process() in {}'.format(self.getClassName())) \n",
    "        \n",
    "    def run(self):\n",
    "        self.process()\n",
    "        return self  \n",
    "    \n",
    "class Load(Process):\n",
    "    def __init__(self, import_file_name):\n",
    "        # import_file_name is  full local file name or url to source\n",
    "        self.import_file_name=import_file_name\n",
    "        self.dataframe=None\n",
    "        \n",
    "class LoadDrains(Load):\n",
    "    #def __init__(self, import_file_name):\n",
    "    #    self.import_file_name=import_file_name\n",
    "    \n",
    "    def get_app_name(self):\n",
    "        '''\n",
    "        returns application name from script path\n",
    "        '''\n",
    "        scripts_path = os.getcwd()\n",
    "        rc = ''\n",
    "        pth = scripts_path.split('/')\n",
    "        rc = pth[len(pth)-1]\n",
    "        return rc\n",
    "\n",
    "    def get_repo_folder(self):\n",
    "        '''\n",
    "        returns path to the repo folder from script path\n",
    "        '''\n",
    "        scripts_path = os.getcwd()\n",
    "        rc = ''\n",
    "        rc = scripts_path.replace('/' + self.get_app_name(), '').replace('/scripts','')\n",
    "        return rc\n",
    "    \n",
    "    def get_raw_data_folder(self):\n",
    "        '''\n",
    "        returns path to raw data from script path\n",
    "        '''\n",
    "        scripts_path = os.getcwd()\n",
    "        return self.get_repo_folder() + '/raw-data/' + self.get_app_name()\n",
    "    \n",
    "    def filename(self, in_f):\n",
    "        ps = in_f.split('/')\n",
    "        return ps[len(ps)-1]\n",
    "    \n",
    "    def get_dataframe(self):\n",
    "        return self.dataframe\n",
    "    \n",
    "    def process(self):\n",
    "        print('* Load Drains', self.filename(self.import_file_name))\n",
    "        # print(' - ', self.filename(self.import_file_name))\n",
    "\n",
    "        '''\n",
    "        import_file_name is the full path and name of import file\n",
    "        returns the original raw data as pandas dataframe\n",
    "        '''\n",
    "        self.dataframe = pd.read_csv(self.import_file_name)\n",
    "    \n",
    "\n",
    "class LoadDataWorld(Load):\n",
    "    '''\n",
    "    creates a dataframe with a fresh copy of the data.world dataset \n",
    "    dont forget to run\n",
    "    '''\n",
    "    \n",
    "    def get_dataframe(self):\n",
    "        return self.dataframe\n",
    "    \n",
    "    def process(self):\n",
    "        print('* Load Data.World')\n",
    "        '''\n",
    "        import_file_name is the full path and name of import file\n",
    "        returns the original raw data as pandas dataframe\n",
    "        '''\n",
    "        # download to ~/.dw/cache/{}/latest/data/grb_drains.csv\n",
    "        self.dataframe = dw.load_dataset(self.import_file_name, auto_update=True)\n",
    "        fstr = '~/.dw/cache/{}/latest/data/grb_drains.csv'.format('citizenlabs/grb-storm-drains-2019-04-03')\n",
    "        # \n",
    "        self.dataframe = pd.read_csv(fstr)\n",
    "        \n",
    "#test_import_file_name = '/Users/jameswilfong/Documents/Github/Wilfongjt/01-AAD-data-world/01-In-Progress/00-03-load-spring-lake/data.world/raw-data/adopt-a-drain/CatchBasins_7_17_2019.xls.csv'\n",
    "# assert Load(testfile).get_app_name() == 'adopt-a-drain'\n",
    "#print(Load().get_repo_folder())\n",
    "#print(Load().get_raw_data_folder())\n",
    "#print(helper.get_raw_files('csv'))\n",
    "# assert Load().get_repo_folder() == 'adopt-a-drain'\n",
    "#\n",
    "#df=LoadCSV(test_import_file_name).run().get_dataframe()\n",
    "#df.info\n",
    "#df.head\n",
    "\n",
    "\n",
    "#df=LoadDW('citizenlabs/grb-storm-drains-2019-04-03').run().get_dataframe()\n",
    "\n",
    "#print(df)\n",
    "#df.info\n",
    "#df.head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Condense(Process):\n",
    "    def __init__(self, dataframe, expected_output_columns_list, extraColumns, outlier_settings):\n",
    "        self.dataframe = dataframe\n",
    "        self.expected_output_columns_list=expected_output_columns_list\n",
    "        self.extraColumns = extraColumns\n",
    "        self.outlier_settings = outlier_settings\n",
    "        \n",
    "    def validateColumns(self):  \n",
    "        print(' - validate columns')\n",
    "        '''\n",
    "        check 's column name for the expected colnames\n",
    "        '''\n",
    "        for nm in self.expected_output_columns_list:\n",
    "            if not nm in self.get_dataframe().columns.values:\n",
    "                raise Exception('{} is missing from '.format(get_dataframe().format(nm)) )\n",
    "                \n",
    "    def removeExtraColumns(self):\n",
    "        print(' - Remove extra columns')\n",
    "        for colname in self.extraColumns:\n",
    "            if( colname in self.get_dataframe().columns.values):\n",
    "                print(' -- drop column: ',colname)\n",
    "                self.set_dataframe(self.get_dataframe().drop([colname], axis=1))\n",
    "                \n",
    "    def remove_obvious_outliers(self):\n",
    "        print(' - remove outliers')\n",
    "        '''\n",
    "        remove individual observations\n",
    "        remove range of observation\n",
    "        _outliers is \n",
    "        {\n",
    "          'outliers': [\n",
    "            {'column':'scheduled_day',\n",
    "             'range':(pd.to_datetime('2016-01-01'), pd.to_datetime('2017-01-01')),\n",
    "             'reason':'Remove 2015. Appointment in 2015 has many gaps in the timeline numbers'},\n",
    "            {'column': 'scheduled_day_of_week',\n",
    "             'range': (0,4) ,\n",
    "             'reason':'Remove Saturday and Sunday visits. These are so few that they could easily .'},\n",
    "            {'column':'lon',\n",
    "             'range':(-50.0,-35.0),\n",
    "             'reason':'Remove neighbourhoods that have bad longitudes (too far east).'},\n",
    "            {'column':'scheduled_hour',\n",
    "             'range':(7,20),\n",
    "             'reason':'Remove small number of observations at 6:00 and 21:00 hours.'}\n",
    "          ]\n",
    "        }\n",
    "\n",
    "        '''\n",
    "        \n",
    "        for outlier in self.outlier_settings['outliers']:\n",
    "            # pprint(outlier)\n",
    "            col_name = outlier['column']\n",
    "\n",
    "            if 'range' in outlier:\n",
    "\n",
    "                low = outlier['range'][0]\n",
    "                high = outlier['range'][1]\n",
    "                sz = len(self.get_dataframe())\n",
    "\n",
    "                tmp = None\n",
    "                tmp1 = ''\n",
    "\n",
    "                if isinstance(low, np.datetime64):\n",
    "                    self.set_dataframe(\n",
    "                      self.get_dataframe()[(self.get_dataframe()[col_name].to_datetime() >= low) & (self.get_dataframe()[col_name].to_datetime() <= high)]\n",
    "                    )\n",
    "                else:   \n",
    "                    self.set_dataframe(\n",
    "                        self.get_dataframe()[(self.get_dataframe()[col_name] >= low) & (self.get_dataframe()[col_name] <= high)]\n",
    "                    )\n",
    "                outlier[\"count\"] = sz - len(self.get_dataframe())\n",
    "\n",
    "            elif 'categories' in outlier:\n",
    "                _list = outlier['categories']\n",
    "                sz = len(self.get_dataframe())\n",
    "                self.set_dataframe(\n",
    "                    self.get_dataframe()[self.get_dataframe()[col_name].isin(_list)]\n",
    "                )\n",
    "                outlier[\"count\"] = sz - len(self.get_dataframe())\n",
    "            if \"reason\" in outlier:\n",
    "                outlier[\"reason\"] = outlier[\"reason\"].format(  str(outlier[\"count\"]) )\n",
    "\n",
    "\n",
    "    def remove_duplicates(self):\n",
    "        print(' - drop duplicates')\n",
    "        self.set_dataframe(self.get_dataframe().drop_duplicates('dr_asset_id',keep='first'))\n",
    "    \n",
    "    def get_dataframe(self):\n",
    "        return self.dataframe\n",
    "    \n",
    "    def set_dataframe(self, dataframe):\n",
    "        self.dataframe = dataframe    \n",
    "    \n",
    "    def process(self):\n",
    "        print('* Condense')\n",
    "        self.removeExtraColumns()\n",
    "        self.validateColumns()\n",
    "        self.remove_obvious_outliers()\n",
    "        self.remove_duplicates()\n",
    "        #self.set_dataframe()\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Clean(Process):\n",
    "    def __init__(self, df_source, commonNameMap, region_map):\n",
    "        self.dataframe = df_source\n",
    "        self.commonNameMap = commonNameMap\n",
    "        self.region_map=region_map\n",
    "        \n",
    "class CleanDrains(Clean):\n",
    "    \n",
    "    def clean_column_names(self):\n",
    "        '''\n",
    "        convert each column to lowercase with underscore seperation\n",
    "\n",
    "        e.g., ID to id\n",
    "        e.g., County ID to county_id\n",
    "        e.g., County-ID to county_id\n",
    "        :param actual_col_list: list of column names\n",
    "        :return: clean list of column names\n",
    "\n",
    "        {\n",
    "          'field-name': {}\n",
    "        }\n",
    "\n",
    "        '''\n",
    "        # start_time = time.time()\n",
    "        \n",
    "        actual_col_list = self.dataframe.columns\n",
    "        clean_column_names = {}\n",
    "        for cn in actual_col_list:\n",
    "        \n",
    "            ncn = cn\n",
    "            # get rid of some unwanted characters\n",
    "\n",
    "            if ' ' in cn:\n",
    "                ncn = cn.replace(' ','_')\n",
    "\n",
    "            if '-' in cn:\n",
    "                ncn = cn.replace('-', '_')\n",
    "\n",
    "            # force first char to lower case\n",
    "            nncn = ncn\n",
    "            ncn = ''\n",
    "            prev_upper = True #False\n",
    "            case = False\n",
    "            camelcase = False\n",
    "            for c in nncn:\n",
    "                if c in 'ABCDEFGHIJKLMNOPQRSTUVWXYZ':\n",
    "                    case = True\n",
    "                    if prev_upper:\n",
    "                        ncn += c.lower()\n",
    "                    else:\n",
    "                        ncn += '_' + c.lower()\n",
    "                        camelcase = True\n",
    "                    prev_upper = True\n",
    "                else:\n",
    "                    ncn += c\n",
    "                    prev_upper = False\n",
    "\n",
    "            clean_column_names[cn]=ncn\n",
    "\n",
    "        return self.dataframe.rename(columns=clean_column_names)\n",
    "        # print('* clean_column_names: {} sec'.format(time.time() - start_time))  # time_taken is in seconds\n",
    "    \n",
    "    \n",
    "    def inferName(self, col_name):\n",
    "        '''\n",
    "        select a column name based on previous names found in file\n",
    "        '''\n",
    "        names = { \n",
    "            \"subtype\": \"dr_subtype\",\n",
    "            \"jurisdicti\": \"dr_jurisdiction\",\n",
    "            \"drain__owner\": \"dr_owner\",\n",
    "            \"owner\":\"dr_owner\",\n",
    "            \"local__id\": \"dr_local_id\",\n",
    "            \"facilityid\": \"dr_facility_id\",\n",
    "            \"drain__jurisdiction\": \"dr_jurisdiction\",\n",
    "            \"subwatershed\": \"dr_subwatershed\",\n",
    "            \"subbasin\": \"dr_subwatershed\",\n",
    "            \"point__x\":\"dr_lon\", \n",
    "            \"long\": \"dr_lon\",\n",
    "            \"point__y\":\"dr_lat\",\n",
    "            \"lat\":\"dr_lat\",\n",
    "            \"soure__id\": \"del_source_id\"}\n",
    "\n",
    "        if not col_name in self.commonNameMap:\n",
    "            # mark madeup names for easy id later\n",
    "            return 'del_{}'.format(col_name)\n",
    "        \n",
    "        return self.commonNameMap[col_name]\n",
    "\n",
    "    def getColumnDict(self):\n",
    "\n",
    "        col_dict = {}\n",
    "        for nm in self.dataframe.columns.values:\n",
    "            col_dict[nm]=self.inferName(nm)  \n",
    "        return col_dict\n",
    "    \n",
    "    def remove_char(self,columnList):\n",
    "        '''\n",
    "        some facillity ids have characters mixed wtih number\n",
    "        we need just the number part\n",
    "        this function removes all characters from the facility id\n",
    "        '''\n",
    "        newList = []\n",
    "\n",
    "        for item in columnList:\n",
    "            fi = ''\n",
    "            for ch in str(item):\n",
    "                if ch in '0123456789':\n",
    "                    fi += ch\n",
    "                else:\n",
    "                    fi += '0'\n",
    "            newList.append(fi)\n",
    "\n",
    "        return newList\n",
    "        \n",
    "    def regional_codes(self, df_source , _owner):\n",
    "        '''\n",
    "        regional codes identify the data's source community\n",
    "        code are added over time. this method checks and throws error not found.\n",
    "        fix by adding new owner and code to list below\n",
    "        '''\n",
    "        #print('regional_code 1')\n",
    "        rc = []\n",
    "\n",
    "        # look at data in in the _owner column\n",
    "        for jur in self.dataframe[_owner]:\n",
    "            # check if jur is in the codes\n",
    "            if jur in self.region_map:\n",
    "                rc.append(self.region_map[jur])\n",
    "            else:\n",
    "                print('bad name', )\n",
    "                raise Exception('Regional-Code for ({}) is not available... add new '.format(jur)) \n",
    "                #rc.append('XXX')\n",
    "\n",
    "        return rc    \n",
    "    \n",
    "    def get_dataframe(self):\n",
    "        return self.dataframe\n",
    "    \n",
    "    def process(self):\n",
    "        '''\n",
    "        clean up the df_source\n",
    "        '''\n",
    "        self.dataframe = self.clean_column_names()\n",
    "        self.dataframe = self.dataframe.rename(columns=self.getColumnDict())\n",
    "        \n",
    "        # patch up bad owner and jurisdiction names\n",
    "        self.dataframe['dr_jurisdiction'] = self.dataframe['dr_owner'] # is what it is\n",
    "        \n",
    "        # mark all empties with nan\n",
    "        self.dataframe['dr_facility_id'] = self.dataframe['dr_facility_id'].apply(lambda x:  np.nan if x != x or x == '' or x == ' ' or x == None else x)\n",
    "        \n",
    "        # some dr_facilities have alfa numeric values ... clean up\n",
    "        self.dataframe['dr_facility_id'] = self.remove_char(self.dataframe['dr_facility_id'])\n",
    "        \n",
    "        # add colunm to id the source of data records\n",
    "        self.dataframe['source_code'] = self.regional_codes( self.dataframe , 'dr_owner')\n",
    "        \n",
    "        # convert typ to integer\n",
    "        self.dataframe['dr_facility_id'] = self.dataframe['dr_facility_id'].astype('int64')\n",
    "        \n",
    "        # create final id aka dr_asset_id\n",
    "        self.dataframe['dr_asset_id'] = self.dataframe['source_code'] + '_'+ self.dataframe['dr_facility_id'].astype(str)\n",
    "\n",
    "        self.dataframe['dr_type'] = self.dataframe['dr_asset_id'].apply(lambda x: 'Storm Water Inlet Drain')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wrangle(Process):\n",
    "    def __init__(self, maps):\n",
    "        self.maps=maps\n",
    "        \n",
    "    def get_dataframe(self):\n",
    "        return self.dataframe\n",
    "    \n",
    "    def set_dataframe(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "    \n",
    "    def xls2csv(self,xls_name):\n",
    "        import xlrd\n",
    "        # generates a csv file from the first sheet in an excel file\n",
    "\n",
    "        wb = xlrd.open_workbook(xls_name)\n",
    "        sh = wb.sheet_by_index(0)\n",
    "        your_csv_file = open('{}.csv'.format(xls_name), 'w', encoding='utf8')\n",
    "\n",
    "        wr = csv.writer(your_csv_file, quoting=csv.QUOTE_ALL)\n",
    "        for rownum in range(sh.nrows):\n",
    "            wr.writerow(sh.row_values(rownum))\n",
    "\n",
    "        your_csv_file.close()\n",
    "\n",
    "    def validateOutputColumns(self):\n",
    "\n",
    "        #expected_lst = ['dr_subtype', 'dr_jurisdiction', 'dr_owner', 'dr_local_id', 'dr_facility_id', 'dr_subwatershed', 'dr_lon', 'dr_lat', 'dr_asset_id', 'dr_type', 'dr_sync_id']\n",
    "        expected_lst = ['facility_prefix', 'dr_subtype', 'dr_jurisdiction', 'dr_owner', 'dr_subwatershed', 'dr_lon', 'dr_lat', 'dr_asset_id', 'dr_type', 'dr_sync_id']\n",
    "\n",
    "        for nm in get_dataframe().columns.values:\n",
    "            if not nm in expected_lst:   \n",
    "                raise Exception('{} is unexpected output for clean data'.format(nm))\n",
    "\n",
    "\n",
    "    def filename(self, in_f):\n",
    "        ps = in_f.split('/')\n",
    "        return ps[len(ps)-1]\n",
    "        \n",
    "    def conversions(self):\n",
    "        for xls in helper.get_raw_files('xls'):\n",
    "            print(' -- convert ', xls)\n",
    "            self.xls2csv(xls)\n",
    "            \n",
    "    def finalCSV(self):\n",
    "        self.get_dataframe().to_csv(local_config[\"local_clean\"], index=False)\n",
    "        \n",
    "    def process(self):\n",
    "        print('* Wrangle')\n",
    "        # get list of raw data files\n",
    "        print(' - raw folder ', helper.get_raw_data_folder())\n",
    "        # print(helper.get_raw_files('csv'))\n",
    "        raw_folder = helper.get_raw_data_folder()\n",
    "        clean_folder = helper.get_clean_data_folder()\n",
    "\n",
    "        concat_list = []\n",
    "        #* load data\n",
    "        #* convert xls to csv\n",
    "        #* fix column names\n",
    "        #* map expected colums to raw-data columns\n",
    "        #* drop drains without a facility id\n",
    "        #* fix column types\n",
    "\n",
    "        self.conversions() # convert excel files to csv\n",
    "\n",
    "        # load these up first\n",
    "        concat_list.append(LoadDataWorld(dw_source).run().get_dataframe())\n",
    "\n",
    "        # load up the files in the raw data folder\n",
    "        for in_f in helper.get_raw_files('csv'):\n",
    "            # print(' - raw: ', self.filename(in_f))\n",
    "            \n",
    "            # LOAD\n",
    "            \n",
    "            self.set_dataframe( LoadDrains(in_f)\\\n",
    "                .run()\\\n",
    "                .get_dataframe())\n",
    "            \n",
    "            # CLEAN\n",
    "            \n",
    "            self.set_dataframe( CleanDrains(self.get_dataframe(), \n",
    "                                            self.maps['commonNameMap'], \n",
    "                                            self.maps['region_map']).run().get_dataframe() )\n",
    "            \n",
    "            # COMPILE\n",
    "            \n",
    "            concat_list.append(self.get_dataframe())\n",
    "                               \n",
    "        '''\n",
    "        --------------------------------- combine datasets\n",
    "        '''      \n",
    "        self.set_dataframe( pd.concat(concat_list) )\n",
    "\n",
    "        '''\n",
    "        --------------------------------- Condense dataset (cols, rows)\n",
    "        '''\n",
    "        self.set_dataframe( Condense(self.get_dataframe(),\\\n",
    "                                     expected_output_columns_list,\\\n",
    "                                     extraColumns,\\\n",
    "                                     outlier_settings).run().get_dataframe() )\n",
    "\n",
    "        '''\n",
    "        --------------------------------- save csv \n",
    "        '''\n",
    "        # assume new file and remove old one\n",
    "        local_config[\"local_clean\"]='{}/{}'.format(helper.get_clean_data_folder(),metadata['output_file_name'])\n",
    "\n",
    "        if os.path.isfile(local_config[\"local_clean\"]):\n",
    "            os.remove(local_config['local_clean'])\n",
    "            cell_log.collect('* deleted {} '.format(local_config['local_clean']))\n",
    "\n",
    "        # stop if columns are not expected\n",
    "        #self.validateOutputColumns(self.get_dataframe())\n",
    "        self.finalCSV()\n",
    "        #df_source.to_csv(local_config[\"local_clean\"], index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrangling Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Wrangle\n",
      " - raw folder  /Users/jameswilfong/Documents/Github/Wilfongjt/01-AAD-data-world/01-In-Progress/04-notebook-grb-drains-validation/data.world/raw-data/adopt-a-drain\n",
      " -- convert  /Users/jameswilfong/Documents/Github/Wilfongjt/01-AAD-data-world/01-In-Progress/04-notebook-grb-drains-validation/data.world/raw-data/adopt-a-drain/CatchBasins_7_17_2019.xls\n",
      "* Load Data.World\n",
      "* Load Drains CatchBasins_7_17_2019.xls.csv\n",
      "* Condense\n",
      " - Remove extra columns\n",
      " -- drop column:  del_source\n",
      " -- drop column:  del_fid\n",
      " -- drop column:  source_code\n",
      " -- drop column:  dr_local_id\n",
      " -- drop column:  dr_facility_id\n",
      " -- drop column:  dr_location\n",
      " - validate columns\n",
      " - remove outliers\n",
      " - drop duplicates\n"
     ]
    }
   ],
   "source": [
    "# NEW CELL\n",
    "# testing \n",
    "# current dataset from dataworld\n",
    "dw_source = 'citizenlabs/grb-storm-drains-2019-04-03'\n",
    "wrangle=Wrangle(maps)\n",
    "cell_log.clear()\n",
    "if ENV_ERROR:\n",
    "    cell_log.collect(\"# Script Failure!!\")\n",
    "    cell_log.collect(\"# !!! Missing Environment Variables !!!\")\n",
    "    cell_log.collect(\"### see [Environment Variable Setup](#env-setup)\")\n",
    "else:\n",
    "    # get list of raw data files\n",
    "    wrangle.run()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# NEW CELL\\n# testing \\n# current dataset from dataworld\\ndw_source = \\'citizenlabs/grb-storm-drains-2019-04-03\\'\\n\\ncell_log.clear()\\nif ENV_ERROR:\\n    cell_log.collect(\"# Script Failure!!\")\\n    cell_log.collect(\"# !!! Missing Environment Variables !!!\")\\n    cell_log.collect(\"### see [Environment Variable Setup](#env-setup)\")\\nelse:\\n    # get list of raw data files\\n    print(\\'raw folder \\', helper.get_raw_data_folder())\\n    # print(helper.get_raw_files(\\'csv\\'))\\n    raw_folder = helper.get_raw_data_folder()\\n    clean_folder = helper.get_clean_data_folder()\\n    \\n    concat_list = []\\n    #* load data\\n    #* convert xls to csv\\n    #* fix column names\\n    #* map expected colums to raw-data columns\\n    #* drop drains without a facility id\\n    #* fix column types\\n    \\n    for xls in helper.get_raw_files(\\'xls\\'):\\n        print(\\'--------\\')\\n        print(xls)\\n        xls2csv(xls)\\n    \\n\\n    # load these up first\\n    concat_list.append(LoadDataWorld(dw_source).run().get_dataframe())\\n    \\n    # load up the files in the raw data folder\\n    for in_f in helper.get_raw_files(\\'csv\\'):\\n        print(\\'-----------------------------------\\')\\n        print(\\'raw: \\', in_f)\\n        print(\\'-----------------------------------\\')\\n        ##################\\n        # LOAD\\n        ######\\n        df_source = LoadDrains(in_f).run().get_dataframe()\\n        ##################\\n        # CLEAN\\n        ######\\n        df_source = CleanDrains(df_source, commonNameMap, region_map)            .run()            .get_dataframe()\\n        \\n        \\n        #--------------------------------- Compile a list of cleaned datasets\\n        \\n        concat_list.append(df_source)\\n    print(\\'done loading files\\')\\n    #removeList, \\n    \\n    cell_log.collect(\"\")\\n    cell_log.collect(\"# Combined\")\\n    \\n    #--------------------------------- combine datasets\\n          \\n    df_source = pd.concat(concat_list)\\n    \\n    \\n    #--------------------------------- Condense dataset (cols, rows)\\n    \\n    scnt = len(df_source)\\n    df_source = Condense(df_source, expected_output_columns_list, extraColumns, outlier_settings).process()\\n    ecnt = len(df_source)\\n    cell_log.collect(\\'* duplicates: dropped {} duplicate asset ids\\'.format(scnt - ecnt))\\n   \\n\\n    \\n    #--------------------------------- save csv \\n    \\n    # assume new file and remove old one\\n    local_config[\"local_clean\"]=\\'{}/{}\\'.format(helper.get_clean_data_folder(),metadata[\\'output_file_name\\'])\\n\\n    if os.path.isfile(local_config[\"local_clean\"]):\\n        os.remove(local_config[\\'local_clean\\'])\\n        cell_log.collect(\\'* deleted {} \\'.format(local_config[\\'local_clean\\']))\\n\\n    cell_log.collect(\"* inter-output: columns {}\".format(df_source.columns.values))\\n    cell_log.collect(\\'* inter-output: {} obs to {}\\'.format(len(df_source) , local_config[\"local_clean\"]))\\n\\n    # stop if columns are not expected\\n    validateOutputColumns(df_source)\\n\\n    df_source.to_csv(local_config[\"local_clean\"], index=False)\\n'"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# NEW CELL\n",
    "# testing \n",
    "# current dataset from dataworld\n",
    "dw_source = 'citizenlabs/grb-storm-drains-2019-04-03'\n",
    "\n",
    "cell_log.clear()\n",
    "if ENV_ERROR:\n",
    "    cell_log.collect(\"# Script Failure!!\")\n",
    "    cell_log.collect(\"# !!! Missing Environment Variables !!!\")\n",
    "    cell_log.collect(\"### see [Environment Variable Setup](#env-setup)\")\n",
    "else:\n",
    "    # get list of raw data files\n",
    "    print('raw folder ', helper.get_raw_data_folder())\n",
    "    # print(helper.get_raw_files('csv'))\n",
    "    raw_folder = helper.get_raw_data_folder()\n",
    "    clean_folder = helper.get_clean_data_folder()\n",
    "    \n",
    "    concat_list = []\n",
    "    #* load data\n",
    "    #* convert xls to csv\n",
    "    #* fix column names\n",
    "    #* map expected colums to raw-data columns\n",
    "    #* drop drains without a facility id\n",
    "    #* fix column types\n",
    "    \n",
    "    for xls in helper.get_raw_files('xls'):\n",
    "        print('--------')\n",
    "        print(xls)\n",
    "        xls2csv(xls)\n",
    "    \n",
    "\n",
    "    # load these up first\n",
    "    concat_list.append(LoadDataWorld(dw_source).run().get_dataframe())\n",
    "    \n",
    "    # load up the files in the raw data folder\n",
    "    for in_f in helper.get_raw_files('csv'):\n",
    "        print('-----------------------------------')\n",
    "        print('raw: ', in_f)\n",
    "        print('-----------------------------------')\n",
    "        ##################\n",
    "        # LOAD\n",
    "        ######\n",
    "        df_source = LoadDrains(in_f).run().get_dataframe()\n",
    "        ##################\n",
    "        # CLEAN\n",
    "        ######\n",
    "        df_source = CleanDrains(df_source, commonNameMap, region_map)\\\n",
    "            .run()\\\n",
    "            .get_dataframe()\n",
    "        \n",
    "        \n",
    "        #--------------------------------- Compile a list of cleaned datasets\n",
    "        \n",
    "        concat_list.append(df_source)\n",
    "    print('done loading files')\n",
    "    #removeList, \n",
    "    \n",
    "    cell_log.collect(\"\")\n",
    "    cell_log.collect(\"# Combined\")\n",
    "    \n",
    "    #--------------------------------- combine datasets\n",
    "          \n",
    "    df_source = pd.concat(concat_list)\n",
    "    \n",
    "    \n",
    "    #--------------------------------- Condense dataset (cols, rows)\n",
    "    \n",
    "    scnt = len(df_source)\n",
    "    df_source = Condense(df_source, expected_output_columns_list, extraColumns, outlier_settings).process()\n",
    "    ecnt = len(df_source)\n",
    "    cell_log.collect('* duplicates: dropped {} duplicate asset ids'.format(scnt - ecnt))\n",
    "   \n",
    "\n",
    "    \n",
    "    #--------------------------------- save csv \n",
    "    \n",
    "    # assume new file and remove old one\n",
    "    local_config[\"local_clean\"]='{}/{}'.format(helper.get_clean_data_folder(),metadata['output_file_name'])\n",
    "\n",
    "    if os.path.isfile(local_config[\"local_clean\"]):\n",
    "        os.remove(local_config['local_clean'])\n",
    "        cell_log.collect('* deleted {} '.format(local_config['local_clean']))\n",
    "\n",
    "    cell_log.collect(\"* inter-output: columns {}\".format(df_source.columns.values))\n",
    "    cell_log.collect('* inter-output: {} obs to {}'.format(len(df_source) , local_config[\"local_clean\"]))\n",
    "\n",
    "    # stop if columns are not expected\n",
    "    validateOutputColumns(df_source)\n",
    "\n",
    "    df_source.to_csv(local_config[\"local_clean\"], index=False)\n",
    "'''\n",
    "#Markdown('''{}'''.format(cell_log.getMarkdown()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output new CSV File\n",
    "* replacement for data.world and the production db\n",
    "* the small version for the test db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jameswilfong/Documents/Github/Wilfongjt/01-AAD-data-world/01-In-Progress/04-notebook-grb-drains-validation/data.world/clean-data/adopt-a-drain/grb_drains-2019-08-024.csv\n",
      "/Users/jameswilfong/Documents/Github/Wilfongjt/01-AAD-data-world/01-In-Progress/04-notebook-grb-drains-validation/data.world/clean-data/adopt-a-drain/grb_drains.csv\n"
     ]
    }
   ],
   "source": [
    "from shutil import copyfile\n",
    "# OUTPUT_FILE_NAME\n",
    "#/Users/jameswilfong/Documents/Github/CitizenLabs/00-Data-World/03-april-data/data.world/clean-data/adopt-a-drain/grb_drains-2019-04-02.csv\n",
    "\n",
    "# ifn = '{}/{}'.format(helper.get_clean_data_folder(), OUTPUT_FILE_NAME)\n",
    "\n",
    "\n",
    "ifn = '{}/{}'.format(helper.get_clean_data_folder(), metadata['output_file_name'])\n",
    "ofn = '{}/{}'.format(helper.get_clean_data_folder(), metadata['copy_file_name'])\n",
    "\n",
    "print(ifn)\n",
    "print(ofn)\n",
    "\n",
    "copyfile(ifn, ofn)\n",
    "# set up a smaller version of file\n",
    "tfn = '{}/{}'.format(helper.get_test_version_folder(), metadata['copy_file_name'])\n",
    "#df_small = df_source.query(\"dr_jurisdiction = 'City of Grand Rapids'\")\n",
    "df_small=wrangle.get_dataframe().query(\"dr_jurisdiction == 'City of Grand Rapids'\").head(5000)\n",
    "df_small.to_csv( tfn, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Appendix - Data.World Names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keeping the names straight\n",
    "\n",
    "| CSV Name      | Table Name    | Title          | Dataset ID      | Restful |\n",
    "| :------------ |:------------- | :------------- | :-------------  | :------------- |\n",
    "| xxxx_xx       | xxxx_xx       | Xxxx Xx        | xxxx-xx         |    ?     | \n",
    "| xxxx_xx       | xxxx_xx       | Xxxx_Xx        | xxxxxx          |    ?            |\n",
    "| xxxx_xx       | xxxx_xx       | Xxxx-Xx        | xxxx-xx         |    ?         |\n",
    "| xxxx-xx       | xxxx_xx       | Xxxx Xx        | xxxx-xx         |    ?         |\n",
    "| xxxx-xx       | xxxx_xx       | Xxxx_Xx        | xxxxxx          |    ?         |\n",
    "| xxxx-xx       | xxxx_xx       | Xxxx-Xx        | xxxx-xx         |    ?         |\n",
    "\n",
    "* CSV Name is root of Table name\n",
    "* Title is root of Dataset ID\n",
    "* a space in Title will be automatically converted to hyphen in dataset id\n",
    "* an underscore in Title will be removed in Dataset ID\n",
    "* a hyphen in CSV Name will be replaced with underscore in Table Name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
