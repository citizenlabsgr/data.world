{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Project: Adopt a Drain Data Cleaning\n",
    " * Author: James Wilfong, wilfongjt@gmail.com\n",
    " ## Limits\n",
    " * This script doesn't interface with GitHub or Data.world. That happens in the maintainer/maintainer.Process.ipynb script.\n",
    " ## Input\n",
    " * The input data is contained in the raw-data/adopt-a-drain folder of this repo. \n",
    " * The cleaning process is initiated by running this Jupyter Notebook.\n",
    " * The cleaned data is put in the clean-data folder of this repo.\n",
    " ## Process\n",
    " * load dataset(s) from raw-data/adopt-a-drain folder\n",
    " * create source id for temporary use, drop later\n",
    " * create an asset number from facillity id\n",
    " * create a sync id,  \n",
    " * ensure input columns are the expected columns\n",
    " * remove outliers\n",
    " * concat multiple datasets, Make on big dataset from one or many\n",
    " * remove duplicate facility ids (keep the first of duplicates)\n",
    " * save combined datasets to clean-data/adopt-a-drain/clean.csv\n",
    " ## Output\n",
    " * Clean data is output to the clean-data/adopt-a-drain folder of the repo.\n",
    " ## Next Steps\n",
    " * Clean data is pushed to repo by the developer.\n",
    " * The developer creates a pull request for the maintainer.\n",
    " * The Maintainer will complete the task of loading data into production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTANT\n",
    "## Configuring the Data Transfer\n",
    "Configure before running \"RUN All\" in the Cell menu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUT_FILE_NAME:  grb_drains-2019-04-05.csv\n",
      "kill log:  ./maintainer/maintainer-config.json\n"
     ]
    }
   ],
   "source": [
    "import helper\n",
    "'''\n",
    "    Input: CSV file in raw_data/ folder\n",
    "    Process: clean (conform, condence)\n",
    "    Output: is directed to the clean-data/ folder\n",
    "    \n",
    "    Name the output file using OUTPUT_FILE\n",
    "    OUTPUT_FILELOCAL_CLEAN_NAME is used to name the data.world table\n",
    "    Table names should start with letter, may contain letters, numbers, underscores\n",
    "    \n",
    "'''\n",
    "\n",
    "\n",
    "#INPUT_FILE_NAME='KentCoCatchBasinWatershedJoin_Filtered.csv' # input file is found in raw-data/ folder\n",
    "#OUTPUT_FILE_NAME='KentCoCatchBasinWatershedJoin_Filtered.csv' # output file is found in clean-data/ folder. Name is used as data.world table name\n",
    "\n",
    "#INPUT_FILE_NAME='grb_drains.csv' # input file is found in raw-data/ folder\n",
    "#OUTPUT_FILE_NAME='grb_drains.csv' # output file is found in clean-data/ folder. Name is used as data.world table name\n",
    "\n",
    "#OUTPUT_FILE_NAME='clean.csv'\n",
    "OUTPUT_FILE_NAME='{}-{}.csv'.format('grb_drains', helper.get_daystamp(),'csv')\n",
    "#OUTPUT_FILE_NAME='{}.csv'.format('grb_drains')\n",
    "COPY_FILE_NAME='grb_drains.csv'\n",
    "print('OUTPUT_FILE_NAME: ', OUTPUT_FILE_NAME)\n",
    "gh_file_type = 'csv'\n",
    "title = 'GRB Storm Drains' # title of data.world dataset. title is also used to name d.w data service\n",
    "desc = 'Storm Drains of the Grand River Basin, Michigan' # describes contents of dataset, appears in d.w  \n",
    "table_name=OUTPUT_FILE_NAME.replace('.csv','')\n",
    "# write the maintainer_config to a file for the maintainer\n",
    "helper.exportMaintainerConfig(OUTPUT_FILE_NAME, gh_file_type, title, desc, table_name)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Assemble Names of:\n",
    "        Application,\n",
    "        Raw data file,\n",
    "        Clean data file\n",
    "'''\n",
    "#LOCAL_RAW_FILE = helper.get_raw_data_folder() + '/{}'.format(INPUT_FILE_NAME )\n",
    "#LOCAL_CLEAN_FILE = helper.get_clean_data_folder() + '/{}'.format(OUTPUT_FILE_NAME)\n",
    "local_config = { \n",
    "                 \"app_name\": helper.get_app_name(),\n",
    "                 \"local_raw\": None,\n",
    "                 \"local_clean\": None\n",
    "               }\n",
    "\n",
    "'''\n",
    "    ------------- configure outliers\n",
    "'''\n",
    "_outliers = {\n",
    "  'outliers': [\n",
    "    {'column':'dr_lon',\n",
    "     'range':(-90.0, -80.0),\n",
    "     'reason':'Remove {} observations too far west or east.',\n",
    "     'count': 0\n",
    "    },  \n",
    "    {'column':'dr_lat',\n",
    "     'range':(40.0, 50.0),\n",
    "     'reason':'Remove {} observations too far north or south.',\n",
    "     'count': 0\n",
    "    }\n",
    "  ]\n",
    "}  \n",
    "ENV_ERROR=False\n",
    "#print('Local_RAW_FILE: ', LOCAL_RAW_FILE)\n",
    "#print('LOCAL_CLEAN_FILE: ',LOCAL_CLEAN_FILE )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "from IPython.display import display, HTML\n",
    "from IPython.display import Markdown\n",
    "\n",
    "from lib.p3_ProcessLogger import ProcessLogger\n",
    "cell_log = ProcessLogger() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "* Import third party packages\n",
       "* Import custom packages"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell_log.clear()\n",
    "#import interface\n",
    "cell_log.collect('* Import third party packages')\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "import pprint\n",
    "\n",
    "import csv # read and write csv files\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import os\n",
    "# import subprocess\n",
    "\n",
    "# convenience functions -- cleaning\n",
    "cell_log.collect('* Import custom packages')\n",
    "from lib.p3_CellCounts import CellCounts\n",
    "import lib.p3_clean as clean\n",
    "from lib.p3_configuration import get_configuration\n",
    "import lib.p3_explore as explore\n",
    "import lib.p3_gather as gather # gathering functions\n",
    "# import lib.p3_helper_functions as helper\n",
    "import lib.p3_map as maps\n",
    "\n",
    "if ENV_ERROR:\n",
    "    cell_log.collect(\"# Script Failure!!\")\n",
    "    cell_log.collect(\"# !!! Missing Environment Variables !!!\")\n",
    "    cell_log.collect(\"### see [Environment Variable Setup](#env-setup)\")\n",
    "\n",
    "Markdown('''{}'''.format(cell_log.getMarkdown()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def inferName(col_name):\n",
    "    names = { \n",
    "        \"subtype\": \"dr_subtype\",\n",
    "        \"jurisdicti\": \"dr_jurisdiction\",\n",
    "        \"drain__owner\": \"dr_owner\",\n",
    "        \"owner\":\"dr_owner\",\n",
    "        \"local__id\": \"dr_local_id\",\n",
    "        \"facilityid\": \"dr_facility_id\",\n",
    "        \"drain__jurisdiction\": \"dr_jurisdiction\",\n",
    "        \"subwatershed\": \"dr_subwatershed\",\n",
    "        \"subbasin\": \"dr_subwatershed\",\n",
    "        \"point__x\":\"dr_lon\", \n",
    "        \"long\": \"dr_lon\",\n",
    "        \"point__y\":\"dr_lat\",\n",
    "        \"lat\":\"dr_lat\",\n",
    "        \"soure__id\": \"del_source_id\"}\n",
    "    \n",
    "    if not col_name in names:\n",
    "        #return names[col_name]\n",
    "        #print('Undefined ', col_name)\n",
    "        #raise Exception('Undefined {} in inferName(col_name)'.format(col_name))\n",
    "        return 'del_{}'.format(col_name)\n",
    "    return names[col_name]\n",
    "    \n",
    "# print('inferName: ', inferName('local__id') )\n",
    "# print('inferName: ', inferName('xxxx') )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getColumnDict(df_source):\n",
    "    \n",
    "    col_dict = {}\n",
    "    for nm in df_source.columns.values:\n",
    "        col_dict[nm]=inferName(nm)\n",
    "        \n",
    "    #print('getColumnDict: ', col_dict)    \n",
    "    return col_dict\n",
    "\n",
    "def validateOutputColumns(df_source):\n",
    "\n",
    "    #expected_lst = ['dr_subtype', 'dr_jurisdiction', 'dr_owner', 'dr_local_id', 'dr_facility_id', 'dr_subwatershed', 'dr_lon', 'dr_lat', 'dr_asset_id', 'dr_type', 'dr_sync_id']\n",
    "    expected_lst = ['dr_subtype', 'dr_jurisdiction', 'dr_owner', 'dr_subwatershed', 'dr_lon', 'dr_lat', 'dr_asset_id', 'dr_type', 'dr_sync_id']\n",
    "    \n",
    "    #print('expected_lst: ',expected_lst)\n",
    "    #print('actual:       ',df_source.columns.values)\n",
    "    \n",
    "    for nm in df_source.columns.values:\n",
    "        if not nm in expected_lst:   \n",
    "            raise Exception('{} is unexpected output for clean data'.format(nm))\n",
    "            \n",
    "def validateInputColumns(df_source, source_file_name):   \n",
    "    #expected_lst = ['del_source_id','dr_subtype', 'dr_jurisdiction', 'dr_owner', 'dr_local_id', 'dr_facility_id', 'dr_subwatershed', 'dr_lon', 'dr_lat', 'dr_asset_no', 'dr_type', 'dr_sync_id']\n",
    "    #expected_lst = ['dr_subtype', 'dr_jurisdiction', 'dr_owner', 'dr_local_id', 'dr_facility_id', 'dr_subwatershed', 'dr_lon', 'dr_lat', 'dr_asset_no', 'dr_type', 'dr_sync_id']\n",
    "    #expected_lst = ['dr_subtype', 'dr_jurisdiction', 'dr_owner', 'dr_local_id', 'dr_facility_id', 'dr_subwatershed', 'dr_lon', 'dr_lat', 'dr_asset_id', 'dr_type', 'dr_sync_id']\n",
    "    #expected_lst = ['dr_subtype', 'dr_jurisdiction', 'dr_owner', 'dr_local_id', 'dr_facility_id', 'dr_subwatershed', 'dr_lon', 'dr_lat', 'dr_asset_id', 'dr_type']\n",
    "    expected_lst = ['dr_subtype', 'dr_jurisdiction', 'dr_owner', 'dr_subwatershed', 'dr_lon', 'dr_lat', 'dr_asset_id', 'dr_type']\n",
    "\n",
    "\n",
    "\n",
    "    for nm in expected_lst:\n",
    "        if not nm in df_source.columns.values:\n",
    "            raise Exception('{} is missing from {}'.format(nm, source_file_name))      \n",
    "\n",
    "    \n",
    "def regional_codes( df_source , _owner):\n",
    "    '''\n",
    "    code are added over time. this method checks and throws error not found.\n",
    "    fix by adding new jurisdiction and code to list below\n",
    "    '''\n",
    "    #print('regional_code 1')\n",
    "    rc = []\n",
    "    codes = {\n",
    "        \"Kent County Road Commission\": \"KCRC\",\n",
    "        \"KENT COUNTY ROAD COMMISSION\":\"KCRC\",\n",
    "        \"City of East Grand Rapids\": \"EGR\",\n",
    "        \"City of Grandville\": \"GRANDV\",\n",
    "        \"City of Wyoming\": \"CWY\",\n",
    "        \"City of Kentwood\": \"CK\",\n",
    "        \"Grand Rapids Township\": \"GRTWP\",\n",
    "        \"City of Walker\": \"CW\",\n",
    "        \"CGR\": \"CGR\",\n",
    "        \"City of Grand Rapids\": \"CGR\",\n",
    "        \"Georgetown Township\": \"GTWP\",\n",
    "        \"City of Hudsonville\": \"CHV\",\n",
    "        \"Jamestown Township\": \"JTTWP\",\n",
    "        \"Cascade Township\": \"CASTWP\",\n",
    "        \"Algoma Township\": \"ALGTWP\",\n",
    "        \"Grattan Township\": \"GRATWP\",\n",
    "        \"Gaines Township\": \"GAITWP\",\n",
    "        \"Vergennes Township\": \"VERTWP\",\n",
    "        \"Lowell Township\": \"LOWTWP\",\n",
    "        \"Oakfield Township\": \"OAKTWP\",\n",
    "        \"Cannon Township\": \"CANTWP\",\n",
    "        \"Sparta Township\": \"SPATWP\",\n",
    "        \"Solon Township\": \"SOLTWP\",\n",
    "        \"Ada Township\": \"ADATWP\",\n",
    "        \"City of Lowell\": \"CLO\",\n",
    "        \"Bowne Township\": \"BOWTWP\",\n",
    "        \"Tyrone Township\": \"TYRTWP\",\n",
    "        \"Caledonia Township\": \"CALTWP\",\n",
    "        \"Courtland Township\": \"COUTWP\",\n",
    "        \"Spencer Township\": \"SPETWP\",\n",
    "        \"Village of Sparta\": \"VSP\",\n",
    "        \"BYRON TOWNSHIP\": \"BYRTWP\",\n",
    "        \"CALEDONIA TOWNSHIP\": \"CALETWP\",\n",
    "        \"City of Rockford\": \"CRF\",\n",
    "        \"Alpine Township\": \"ALPTWP\",\n",
    "        \"Plainfield Township\": \"PLATWP\",\n",
    "        \"Byron Township\": \"BYRTWP\",\n",
    "        \"OCWRC\": \"OCWRC\",\n",
    "        \"City of Grand Haven DPW\":\"CGH\"\n",
    "    }\n",
    "\n",
    "    for jur in df_source[_owner]:\n",
    "        \n",
    "        if jur in codes:\n",
    "            rc.append(codes[jur])\n",
    "        else:\n",
    "            #raise Exception('Regional-Code for ({}) is not available... add new '.format(jur)) \n",
    "            rc.append('XXX')\n",
    "    \n",
    "        \n",
    "    return rc\n",
    "    \n",
    "    \n",
    "'''\n",
    "\n",
    "def regional_codes( df_source , _owner, _facility_id , _local_id ):\n",
    "    \n",
    "    #code are added over time. this method checks and throws error not found.\n",
    "    #fix by adding new jurisdiction and code to list below\n",
    "    \n",
    "    #print('regional_code 1')\n",
    "    rc = []\n",
    "    codes = {\n",
    "        \"Kent County Road Commission\": \"KCRC\",\n",
    "        \"KENT COUNTY ROAD COMMISSION\":\"KCRC\",\n",
    "        \"City of East Grand Rapids\": \"EGR\",\n",
    "        \"City of Grandville\": \"GRANDV\",\n",
    "        \"City of Wyoming\": \"CWY\",\n",
    "        \"City of Kentwood\": \"CK\",\n",
    "        \"Grand Rapids Township\": \"GRTWP\",\n",
    "        \"City of Walker\": \"CW\",\n",
    "        \"CGR\": \"CGR\",\n",
    "        \"City of Grand Rapids\": \"CGR\",\n",
    "        \"Georgetown Township\": \"GTWP\",\n",
    "        \"City of Hudsonville\": \"CHV\",\n",
    "        \"Jamestown Township\": \"JTTWP\",\n",
    "        \"Cascade Township\": \"CASTWP\",\n",
    "        \"Algoma Township\": \"ALGTWP\",\n",
    "        \"Grattan Township\": \"GRATWP\",\n",
    "        \"Gaines Township\": \"GAITWP\",\n",
    "        \"Vergennes Township\": \"VERTWP\",\n",
    "        \"Lowell Township\": \"LOWTWP\",\n",
    "        \"Oakfield Township\": \"OAKTWP\",\n",
    "        \"Cannon Township\": \"CANTWP\",\n",
    "        \"Sparta Township\": \"SPATWP\",\n",
    "        \"Solon Township\": \"SOLTWP\",\n",
    "        \"Ada Township\": \"ADATWP\",\n",
    "        \"City of Lowell\": \"CLO\",\n",
    "        \"Bowne Township\": \"BOWTWP\",\n",
    "        \"Tyrone Township\": \"TYRTWP\",\n",
    "        \"Caledonia Township\": \"CALTWP\",\n",
    "        \"Courtland Township\": \"COUTWP\",\n",
    "        \"Spencer Township\": \"SPETWP\",\n",
    "        \"Village of Sparta\": \"VSP\",\n",
    "        \"BYRON TOWNSHIP\": \"BYRTWP\",\n",
    "        \"CALEDONIA TOWNSHIP\": \"CALETWP\",\n",
    "        \"City of Rockford\": \"CRF\",\n",
    "        \"Alpine Township\": \"ALPTWP\",\n",
    "        \"Plainfield Township\": \"PLATWP\",\n",
    "        \"Byron Township\": \"BYRTWP\",\n",
    "        \"OCWRC\": \"OCWRC\",\n",
    "        \"City of Grand Haven DPW\":\"CGH\"\n",
    "    }\n",
    "\n",
    "    #print('regional_code 2 {}'.format(jurisdiction))\n",
    "    \n",
    "    #for jur in df_source[fld]:\n",
    "    #    #print('jur', jur)\n",
    "    #    if jur in codes:\n",
    "    #        rc.append(codes[jur])\n",
    "    #    else:\n",
    "    #        #raise Exception('Regional-Code for ({}) is not available... add new '.format(jur)) \n",
    "    #        rc.append('XXX_')\n",
    "    \n",
    "    for index, row in df_source.iterrows():\n",
    "        if row[_owner] in codes:\n",
    "            \n",
    "            val = row[_local_id]\n",
    "            key = '{}-{}'.format( codes[row[_owner]], val )\n",
    "            #if pd.isna(row['FACILITYID']):\n",
    "            #    print('na ', pd.isna(row['FACILITYID']),row['FACILITYID'])\n",
    "                \n",
    "            #if (not pd.isna(row['FACILITYID'])) or (not pd.isnull(row['FACILITYID'])) or (row['FACILITYID']==''):\n",
    "            if not pd.isna(row[_facility_id]):\n",
    "                val = row[_facility_id]\n",
    "                key = '{}-{}'.format( codes[row[_owner]], val )\n",
    "                    \n",
    "            rc.append( key )\n",
    "        else:\n",
    "            #raise Exception('Regional-Code for ({}) is not available... add new '.format(jur)) \n",
    "            rc.append('XXX_')\n",
    "    \n",
    "    print('regional_code out')\n",
    "    return rc\n",
    "'''    \n",
    "\n",
    "def fixFacilityId(df_source,_facility_id , _local_id):\n",
    "    rc = []\n",
    "    for index, row in df_source.iterrows():\n",
    "        \n",
    "        val = 0\n",
    "        if not pd.isna(row[_facility_id]):\n",
    "            val = row[_facility_id]\n",
    "        else:    \n",
    "            if not pd.isna( row[_local_id] ):\n",
    "                if isinstance(row[_local_id], int):\n",
    "                    val = row[_local_id]\n",
    "                \n",
    "        rc.append(val)\n",
    "        \n",
    "    return rc\n",
    "\n",
    "def f_facility_id(x, y):\n",
    "\n",
    "    if not pd.isna(x):\n",
    "        return x\n",
    "    if not pd.isna(y):\n",
    "        if isinstance(y, int):\n",
    "            return y\n",
    "    return 0\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrangling Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw folder  /Users/jameswilfong/Documents/Github/CitizenLabs/00-Data-World/04-add-small-version/data.world/raw-data/adopt-a-drain\n",
      "['/Users/jameswilfong/Documents/Github/CitizenLabs/00-Data-World/04-add-small-version/data.world/raw-data/adopt-a-drain/grb_april_1_3_29_2019.csv']\n",
      "-----------------------------------\n",
      "raw:  /Users/jameswilfong/Documents/Github/CitizenLabs/00-Data-World/04-add-small-version/data.world/raw-data/adopt-a-drain/grb_april_1_3_29_2019.csv\n",
      "-----------------------------------\n",
      "* clean_column_names: 0.01604294776916504 sec\n",
      "* remove_obvious_outliers: 0.008656978607177734 sec\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "# CSV Process: What Happened?\n",
       "* input:  /Users/jameswilfong/Documents/Github/CitizenLabs/00-Data-World/04-add-small-version/data.world/raw-data/adopt-a-drain/grb_april_1_3_29_2019.csv\n",
       "* input: 54091 observations\n",
       "* input: columns ['FID' 'SUBTYPE' 'JURISDICTI' 'OWNER' 'SOURCE' 'LOCAL_ID' 'FACILITYID'\n",
       " 'Lat' 'Long' 'Subbasin' 'GID']\n",
       "* format: Apply a style of lowercase and underscores to column names.\n",
       "* format: convert dr_facility_id column to int64\n",
       "* outlier: Remove 0 observations too far west or east.\n",
       "* outlier: Remove 0 observations too far north or south.\n",
       "\n",
       "# Combined\n",
       "* duplicates: dropped 10074 duplicate asset ids\n",
       "* inter-output: columns ['dr_subtype' 'dr_jurisdiction' 'dr_owner' 'dr_lat' 'dr_lon'\n",
       " 'dr_subwatershed' 'dr_asset_id' 'dr_type']\n",
       "* inter-output: 44017 obs to /Users/jameswilfong/Documents/Github/CitizenLabs/00-Data-World/04-add-small-version/data.world/clean-data/adopt-a-drain/grb_drains-2019-04-05.csv"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NEW CELL\n",
    "# testing \n",
    "cell_log.clear()\n",
    "if ENV_ERROR:\n",
    "    cell_log.collect(\"# Script Failure!!\")\n",
    "    cell_log.collect(\"# !!! Missing Environment Variables !!!\")\n",
    "    cell_log.collect(\"### see [Environment Variable Setup](#env-setup)\")\n",
    "else:\n",
    "    # get list of raw data files\n",
    "    print('raw folder ', helper.get_raw_data_folder())\n",
    "\n",
    "    print(helper.get_raw_files('csv'))\n",
    "    raw_folder = helper.get_raw_data_folder()\n",
    "    clean_folder = helper.get_clean_data_folder()\n",
    "    \n",
    "    concat_list = []\n",
    "    #* load data\n",
    "    #* fix column names\n",
    "    #* map expected colums to raw-data columns\n",
    "    #* drop drains without a facility id\n",
    "    #* fix column types\n",
    "    \n",
    "    for in_f in helper.get_raw_files('csv'):\n",
    "        \n",
    "        print('-----------------------------------')\n",
    "        print('raw: ', in_f)\n",
    "        print('-----------------------------------')\n",
    "        \n",
    "        cell_log.collect(\"\")\n",
    "        cell_log.collect(\"# CSV Process: What Happened?\")\n",
    "        '''\n",
    "        --------------------------------- input\n",
    "        '''\n",
    "        local_config['local_raw'] = in_f\n",
    "        local_config['local_clean'] = helper.get_clean_file(in_f)\n",
    "        \n",
    "        \n",
    "        cell_log.collect(\"* input:  {}\".format( local_config[\"local_raw\"]))\n",
    "        '''\n",
    "        --------------------------------- load data\n",
    "        '''\n",
    "        df_source = helper.open_raw_data(local_config) # open raw-data\n",
    "        \n",
    "\n",
    "        cell_log.collect(\"* input: {} observations\".format(len(df_source)))\n",
    "        cell_log.collect(\"* input: columns {}\".format(df_source.columns.values))\n",
    "        '''\n",
    "        --------------------------------- clean column names\n",
    "        '''\n",
    "        cell_log.collect('* format: Apply a style of lowercase and underscores to column names.')##############################\n",
    "        df_source = clean.clean_column_names(df_source) # column names\n",
    "        '''\n",
    "        --------------------------------- map expected colums to raw-data columns\n",
    "        '''\n",
    "        # pprint( getColumnDict(df_source) )\n",
    "        df_source = df_source.rename(columns=getColumnDict(df_source))\n",
    "        '''\n",
    "        --------------------------------- Drop empty facility id\n",
    "        '''\n",
    "        # mark all empties with nan\n",
    "        df_source['dr_facility_id'] = df_source['dr_facility_id'].apply(lambda x:  np.nan if x != x or x == '' or x == ' ' or x == None else x)\n",
    "        \n",
    "        '''\n",
    "        # empty facility id\n",
    "        scnt = len(df_source)\n",
    "        df_source = df_source.dropna(subset=['dr_facility_id'])\n",
    "        ecnt = len(df_source)\n",
    "        cell_log.collect(\"* clean: dropped {} observations with empty dr_facility_id\".format(scnt - ecnt))\n",
    "       \n",
    "        # empty lat \n",
    "        scnt = len(df_source)\n",
    "        df_source = df_source.dropna(subset=['dr_lon', 'dr_lat'])\n",
    "        ecnt = len(df_source)\n",
    "        cell_log.collect(\"* clean: dropped {} observations with empty dr_lon or dr_lat\".format(scnt - ecnt))\n",
    "        '''\n",
    "        # make sure facility-id has a numeric value.\n",
    "        df_source['dr_facility_id'] = df_source[['dr_facility_id','dr_local_id']].apply(lambda x: f_facility_id(*x), axis=1)\n",
    "        #df_source['dr_facility_id'] = fixFacilityId(df_source,'dr_facility_id' , 'dr_local_id')\n",
    "        \n",
    "        df_source['source_code'] = regional_codes( df_source , 'dr_owner')\n",
    "        \n",
    "        '''\n",
    "        --------------------------------- change column types\n",
    "        '''\n",
    "        cell_log.collect('* format: convert dr_facility_id column to int64')\n",
    "        \n",
    "        df_source['dr_facility_id'] = df_source['dr_facility_id'].astype('int64')\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        --------------------------------- Patch \n",
    "        '''\n",
    "        df_source['dr_owner'] = df_source['dr_owner'].apply(lambda x: x if x!='CGR' else 'City of Grand Rapids')\n",
    "        \n",
    "        '''\n",
    "        --------------------------------- remove numbers from df_source_id\n",
    "        '''\n",
    "        df_source['dr_owner'] = df_source['dr_owner'].apply(lambda x: x if x!='CGR' else 'City of Grand Rapids')\n",
    "        \n",
    "        df_source['dr_jurisdiction'] = df_source['dr_owner'] # is what it is\n",
    "        #df_source['del_source_id'] = df_source['del_source_id'].apply(lambda x: x.split('_')[0] + '_' if isinstance(x, str) else 'XXX_') \n",
    "        \n",
    "       \n",
    "        # df_source['dr_asset_no'] = df_source['dr_facility_id']\n",
    "        #df_source['dr_asset_id'] = df_source[['dr_facility_id','dr_local_id']].apply(lambda x: fasset_id(*x), axis=1)\n",
    "        df_source['dr_asset_id'] = df_source['source_code'] + '_'+ df_source['dr_facility_id'].astype(str)\n",
    "\n",
    "\n",
    "        #df_source['dr_type'] = df_source['dr_facility_id'].apply(lambda x: 'Storm Water Inlet Drain')\n",
    "        df_source['dr_type'] = df_source['dr_asset_id'].apply(lambda x: 'Storm Water Inlet Drain')\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        --------------------------------- create a sync id\n",
    "        '''\n",
    "        #df_source['dr_sync_id'] = df_source['del_source_id'] + df_source['dr_facility_id'].astype(str)\n",
    "      \n",
    "        #df_source['dr_sync_id'] = regional_codes( df_source,'dr_owner','dr_facility_id' ,'dr_local_id' )\n",
    "        #df_source['dr_sync_id'] = regional_codes( df_source,'dr_owner','dr_asset_id' ,'dr_local_id' )\n",
    "        '''\n",
    "        --------------------------------- Condense\n",
    "        '''\n",
    "        #df_source = df_source.drop(['del_source_id'], axis=1)\n",
    "        df_source = df_source.drop(['del_source'], axis=1)\n",
    "        df_source = df_source.drop(['del_fid'], axis=1)\n",
    "        df_source = df_source.drop(['del_gid'], axis=1)\n",
    "        df_source = df_source.drop(['source_code'], axis=1)\n",
    "        df_source = df_source.drop(['dr_local_id'], axis=1)\n",
    "        df_source = df_source.drop(['dr_facility_id'], axis=1)\n",
    "        \n",
    "        '''\n",
    "        --------------------------------- check input cols \n",
    "        '''\n",
    "        validateInputColumns(df_source, local_config[\"local_raw\"])\n",
    "        \n",
    "        '''\n",
    "        --------------------------------- outliers\n",
    "        '''\n",
    "        df_source = clean.remove_obvious_outliers(_outliers, df_source)\n",
    "        for r in _outliers['outliers']:\n",
    "            cell_log.collect('* outlier: {}'.format(r['reason']))\n",
    "        '''\n",
    "        --------------------------------- Concat list\n",
    "        '''\n",
    "        concat_list.append(df_source)\n",
    "    \n",
    "    \n",
    "    cell_log.collect(\"\")\n",
    "    cell_log.collect(\"# Combined\")\n",
    "    '''\n",
    "    --------------------------------- concat \n",
    "    '''      \n",
    "    df_source = pd.concat(concat_list)\n",
    "    \n",
    "    '''\n",
    "    --------------------------------- Drop DUPLICATES\n",
    "    '''\n",
    "    scnt = len(df_source)\n",
    "    #df_source = df_source.drop_duplicates('dr_facility_id',keep='first')\n",
    "    df_source = df_source.drop_duplicates('dr_asset_id',keep='first')\n",
    "    ecnt = len(df_source)\n",
    "    cell_log.collect('* duplicates: dropped {} duplicate asset ids'.format(scnt - ecnt))\n",
    "   \n",
    "    '''\n",
    "    --------------------------------- save csv \n",
    "    '''\n",
    "    # assume new file and remove old one\n",
    "    local_config[\"local_clean\"]='{}/{}'.format(helper.get_clean_data_folder(),OUTPUT_FILE_NAME)\n",
    "\n",
    "    if os.path.isfile(local_config[\"local_clean\"]):\n",
    "        os.remove(local_config['local_clean'])\n",
    "        cell_log.collect('* deleted {} '.format(local_config['local_clean']))\n",
    "\n",
    "    cell_log.collect(\"* inter-output: columns {}\".format(df_source.columns.values))\n",
    "    cell_log.collect('* inter-output: {} obs to {}'.format(len(df_source) , local_config[\"local_clean\"]))\n",
    "\n",
    "    # stop if columns are not expected\n",
    "    validateOutputColumns(df_source)\n",
    "\n",
    "    df_source.to_csv(local_config[\"local_clean\"], index=False)\n",
    "\n",
    "\n",
    "Markdown('''{}'''.format(cell_log.getMarkdown()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jameswilfong/Documents/Github/CitizenLabs/00-Data-World/04-add-small-version/data.world/clean-data/adopt-a-drain/grb_drains-2019-04-05.csv\n",
      "/Users/jameswilfong/Documents/Github/CitizenLabs/00-Data-World/04-add-small-version/data.world/clean-data/adopt-a-drain/grb_drains.csv\n"
     ]
    }
   ],
   "source": [
    "from shutil import copyfile\n",
    "OUTPUT_FILE_NAME\n",
    "#/Users/jameswilfong/Documents/Github/CitizenLabs/00-Data-World/03-april-data/data.world/clean-data/adopt-a-drain/grb_drains-2019-04-02.csv\n",
    "\n",
    "ifn = '{}/{}'.format(helper.get_clean_data_folder(), OUTPUT_FILE_NAME)\n",
    "ofn = '{}/{}'.format(helper.get_clean_data_folder(), COPY_FILE_NAME)\n",
    "\n",
    "print(ifn)\n",
    "print(ofn)\n",
    "\n",
    "copyfile(ifn, ofn)\n",
    "# set up a smaller version of file\n",
    "tfn = '{}/{}'.format(helper.get_test_version_folder(), COPY_FILE_NAME)\n",
    "#df_small = df_source.query(\"dr_jurisdiction = 'City of Grand Rapids'\")\n",
    "df_small=df_source.query(\"dr_jurisdiction == 'City of Grand Rapids'\").head(5000)\n",
    "df_small.to_csv( tfn, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Appendix - Data.World Names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keeping the names straight\n",
    "\n",
    "| CSV Name      | Table Name    | Title          | Dataset ID      | Restful |\n",
    "| :------------ |:------------- | :------------- | :-------------  | :------------- |\n",
    "| xxxx_xx       | xxxx_xx       | Xxxx Xx        | xxxx-xx         |    ?     | \n",
    "| xxxx_xx       | xxxx_xx       | Xxxx_Xx        | xxxxxx          |    ?            |\n",
    "| xxxx_xx       | xxxx_xx       | Xxxx-Xx        | xxxx-xx         |    ?         |\n",
    "| xxxx-xx       | xxxx_xx       | Xxxx Xx        | xxxx-xx         |    ?         |\n",
    "| xxxx-xx       | xxxx_xx       | Xxxx_Xx        | xxxxxx          |    ?         |\n",
    "| xxxx-xx       | xxxx_xx       | Xxxx-Xx        | xxxx-xx         |    ?         |\n",
    "\n",
    "* CSV Name is root of Table name\n",
    "* Title is root of Dataset ID\n",
    "* a space in Title will be automatically converted to hyphen in dataset id\n",
    "* an underscore in Title will be removed in Dataset ID\n",
    "* a hyphen in CSV Name will be replaced with underscore in Table Name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
