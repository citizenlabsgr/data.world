{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Project: Adopt a Drain Data Cleaning\n",
    " * Author: James Wilfong, wilfongjt@gmail.com\n",
    " * This script doesn't interface with GitHub or Data.world. That happens in the maintainer/maintainer.Process.ipynb script.\n",
    " * The input data is contained in the raw-data folder of this repo. \n",
    " * The cleaning process is initiated by running this Jupyter Notebook.\n",
    " * The cleaned data is put in the clean-data folder of this repo.\n",
    " * Clean data is pushed to repo by the developer.\n",
    " * The developer creates a pull request for the maintainer.\n",
    " * The Maintainer will complete the task of loading data into production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTANT\n",
    "## Configuring the Data Transfer\n",
    "Configure before running \"RUN All\" in the Cell menu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kill log:  ./maintainer/maintainer-config.json\n"
     ]
    }
   ],
   "source": [
    "import helper\n",
    "'''\n",
    "    Input: CSV file in raw_data/ folder\n",
    "    Process: clean (conform, condence)\n",
    "    Output: is directed to the clean-data/ folder\n",
    "    \n",
    "    Name the output file using OUTPUT_FILE\n",
    "    OUTPUT_FILELOCAL_CLEAN_NAME is used to name the data.world table\n",
    "    Table names should start with letter, may contain letters, numbers, underscores\n",
    "    \n",
    "'''\n",
    "INPUT_FILE_NAME='grb_drains.csv' # input file is found in raw-data/ folder\n",
    "OUTPUT_FILE_NAME='grb_drains.csv' # output file is found in clean-data/ folder. Name is used as data.world table name\n",
    "gh_file_type = 'csv'\n",
    "title = 'GRB Storm Drains' # title of data.world dataset. title is also used to name d.w data service\n",
    "desc = 'Storm Drains of the Grand River Basin, Michigan' # describes contents of dataset, appears in d.w  \n",
    "table_name=OUTPUT_FILE_NAME.replace('.csv','')\n",
    "# write the maintainer_config to a file for the maintainer\n",
    "helper.exportMaintainerConfig(OUTPUT_FILE_NAME, gh_file_type, title, desc, table_name)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Assemble Names of:\n",
    "        Application,\n",
    "        Raw data file,\n",
    "        Clean data file\n",
    "'''\n",
    "LOCAL_RAW_FILE = helper.get_raw_data_folder() + '/{}'.format(INPUT_FILE_NAME )\n",
    "LOCAL_CLEAN_FILE = helper.get_clean_data_folder() + '/{}'.format(OUTPUT_FILE_NAME)\n",
    "local_config = { \n",
    "                 \"app_name\": helper.get_app_name(),\n",
    "                 \"local_raw\": LOCAL_RAW_FILE,\n",
    "                 \"local_clean\": LOCAL_CLEAN_FILE,\n",
    "               }\n",
    "\n",
    "'''\n",
    "    ------------- configure outliers\n",
    "'''\n",
    "_outliers = {\n",
    "  'outliers': [\n",
    "    {'column':'dr_facility_id',\n",
    "     'range':(1, 50000000),\n",
    "     'reason':'ignore {} outliers (1 <= dr_facility_id or => 50000000).',\n",
    "     'count': 0\n",
    "    }, \n",
    "    {'column':'dr_lon',\n",
    "     'range':(-90.0, -80.0),\n",
    "     'reason':'Remove {} observations too far west or east.',\n",
    "     'count': 0\n",
    "    },  \n",
    "    {'column':'dr_lat',\n",
    "     'range':(40.0, 50.0),\n",
    "     'reason':'Remove {} observations too far north or south.',\n",
    "     'count': 0\n",
    "    }\n",
    "  ]\n",
    "}  \n",
    "ENV_ERROR=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "from IPython.display import display, HTML\n",
    "from IPython.display import Markdown\n",
    "\n",
    "from lib.p3_ProcessLogger import ProcessLogger\n",
    "cell_log = ProcessLogger() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "* Import third party packages\n",
       "* Import custom packages"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell_log.clear()\n",
    "import interface\n",
    "cell_log.collect('* Import third party packages')\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "import pprint\n",
    "\n",
    "import csv # read and write csv files\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import os\n",
    "# import subprocess\n",
    "\n",
    "# convenience functions -- cleaning\n",
    "cell_log.collect('* Import custom packages')\n",
    "from lib.p3_CellCounts import CellCounts\n",
    "import lib.p3_clean as clean\n",
    "from lib.p3_configuration import get_configuration\n",
    "import lib.p3_explore as explore\n",
    "import lib.p3_gather as gather # gathering functions\n",
    "# import lib.p3_helper_functions as helper\n",
    "import lib.p3_map as maps\n",
    "\n",
    "if ENV_ERROR:\n",
    "    cell_log.collect(\"# Script Failure!!\")\n",
    "    cell_log.collect(\"# !!! Missing Environment Variables !!!\")\n",
    "    cell_log.collect(\"### see [Environment Variable Setup](#env-setup)\")\n",
    "\n",
    "Markdown('''{}'''.format(cell_log.getMarkdown()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrangling Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jameswilfong/Documents/Github/CitizenLabs/data.world/raw-data/adopt-a-drain/catch basins.csv\n",
      "* clean_column_names: 0.004213094711303711 sec\n",
      "* remove_obvious_outliers: 0.009351015090942383 sec\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# CSV Process: What Happened?\n",
       "* input:  /Users/jameswilfong/Documents/Github/CitizenLabs/data.world/raw-data/adopt-a-drain/catch basins.csv\n",
       "* input: 40204 observations\n",
       "* input: columns ['SUBTYPE' 'DRAIN_JURISDICTION' 'DRAIN_OWNER' 'Soure_ID' 'LOCAL_ID'\n",
       " 'FACILITYID' 'Subwatershed' 'POINT_X' 'POINT_Y']\n",
       "* format: Apply a style of lowercase and underscores to column names.\n",
       "* clean: dropped 15 observations with empty dr_facility_id, soure___id, dr_lon, or dr_lat\n",
       "* format: convert dr_facility_id column to int64\n",
       "* outlier: ignore 0 outliers (1 <= dr_facility_id or => 50000000).\n",
       "* outlier: Remove 0 observations too far west or east.\n",
       "* outlier: Remove 0 observations too far north or south.\n",
       "* duplicates: dropped 225 duplicate facility ids\n",
       "* inter-output: columns ['dr_subtype' 'dr_jurisdiction' 'dr_owner' 'dr_local_id' 'dr_facility_id'\n",
       " 'dr_subwatershed' 'dr_lon' 'dr_lat' 'dr_asset_no' 'dr_type' 'dr_sync_id']\n",
       "* inter-output: 39964 obs to /Users/jameswilfong/Documents/Github/CitizenLabs/data.world/clean-data/adopt-a-drain/grb_drains.csv"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell_log.clear()\n",
    "if ENV_ERROR:\n",
    "    cell_log.collect(\"# Script Failure!!\")\n",
    "    cell_log.collect(\"# !!! Missing Environment Variables !!!\")\n",
    "    cell_log.collect(\"### see [Environment Variable Setup](#env-setup)\")\n",
    "else:\n",
    "    cell_log.collect(\"# CSV Process: What Happened?\")\n",
    "    '''\n",
    "    --------------------------------- input\n",
    "    '''\n",
    "    print(local_config[\"local_raw\"])\n",
    "    cell_log.collect(\"* input:  {}\".format( local_config[\"local_raw\"]))\n",
    "    '''\n",
    "    --------------------------------- load data\n",
    "    '''\n",
    "    df_source = helper.open_raw_data(local_config) # open raw-data\n",
    "    \n",
    "    cell_log.collect(\"* input: {} observations\".format(len(df_source)))\n",
    "    cell_log.collect(\"* input: columns {}\".format(df_source.columns.values))\n",
    "\n",
    "    '''\n",
    "    --------------------------------- clean column names\n",
    "    '''\n",
    "    cell_log.collect('* format: Apply a style of lowercase and underscores to column names.')##############################\n",
    "    df_source = clean.clean_column_names(df_source) # column names\n",
    "\n",
    "    '''\n",
    "    --------------------------------- map expected colums to raw-data columns\n",
    "    '''\n",
    "    df_source = df_source.rename(columns={ # rename columns in df\n",
    "        \"subtype\": \"dr_subtype\",\n",
    "        \"drain__owner\": \"dr_owner\",\n",
    "        \"local__id\": \"dr_local_id\",\n",
    "        \"facilityid\": \"dr_facility_id\",\n",
    "        \"drain__jurisdiction\": \"dr_jurisdiction\",\n",
    "        \"subwatershed\": \"dr_subwatershed\",\n",
    "        \"point__x\":\"dr_lon\", \n",
    "        \"point__y\":\"dr_lat\"})\n",
    "\n",
    "    '''\n",
    "    --------------------------------- change empty values\n",
    "    '''\n",
    "\n",
    "    ## ------------------------------ DROP empty Facility id\n",
    "    # mark all empties with same value\n",
    "    df_source['dr_facility_id'] = df_source['dr_facility_id'].apply(lambda x:  np.nan if x != x or x == '' or x == ' ' or x == None else x)\n",
    "    scnt = len(df_source)\n",
    "    df_source = df_source.dropna(subset=['dr_facility_id', 'soure__id','dr_lon', 'dr_lat'])\n",
    "    ecnt = len(df_source)\n",
    "    cell_log.collect(\"* clean: dropped {} observations with empty dr_facility_id, soure___id, dr_lon, or dr_lat\".format(scnt - ecnt))\n",
    "\n",
    "    '''\n",
    "    --------------------------------- change column types\n",
    "    '''\n",
    "    cell_log.collect('* format: convert dr_facility_id column to int64')\n",
    "    df_source['dr_facility_id'] = df_source['dr_facility_id'].astype('int64')\n",
    "\n",
    "    '''\n",
    "    --------------------------------- remove numbers from df_source_id\n",
    "    '''\n",
    "\n",
    "    df_source['soure__id'] = df_source['soure__id'].apply(lambda x: x.split('_')[0] + '_' if isinstance(x, str) else 'XXX_') \n",
    "\n",
    "    df_source['dr_asset_no'] = df_source['dr_facility_id']\n",
    "    df_source['dr_type'] = df_source['dr_facility_id'].apply(lambda x: 'Storm Water Inlet Drain')\n",
    "    '''\n",
    "    --------------------------------- create a sync id\n",
    "    '''\n",
    "    df_source['dr_sync_id'] = df_source['soure__id'] + df_source['dr_facility_id'].astype(str)\n",
    "\n",
    "    '''\n",
    "    --------------------------------- drop soure__id\n",
    "    '''\n",
    "    df_source = df_source.drop(['soure__id'], axis=1)\n",
    "\n",
    "    '''\n",
    "    --------------------------------- outliers\n",
    "    '''\n",
    "    df_source = clean.remove_obvious_outliers(_outliers, df_source)\n",
    "    for r in _outliers['outliers']:\n",
    "        cell_log.collect('* outlier: {}'.format(r['reason']))\n",
    "\n",
    "    '''\n",
    "    --------------------------------- Drop DUPLICATES\n",
    "    '''\n",
    "    scnt = len(df_source)\n",
    "    df_source = df_source.drop_duplicates('dr_facility_id',keep=False)\n",
    "    ecnt = len(df_source)\n",
    "    cell_log.collect('* duplicates: dropped {} duplicate facility ids'.format(scnt - ecnt))\n",
    "\n",
    "    '''\n",
    "    --------------------------------- save csv \n",
    "    '''\n",
    "    # assume new file and remove old one\n",
    "    if os.path.isfile(local_config[\"local_clean\"]):\n",
    "        os.remove(local_config['local_clean'])\n",
    "        cell_log.collect('* deleted {} '.format(local_config['local_clean']))\n",
    "    \n",
    "    cell_log.collect(\"* inter-output: columns {}\".format(df_source.columns.values))\n",
    "    cell_log.collect('* inter-output: {} obs to {}'.format(len(df_source) , local_config[\"local_clean\"]))\n",
    "\n",
    "    df_source.to_csv(local_config[\"local_clean\"], index=False)\n",
    "\n",
    "Markdown('''{}'''.format(cell_log.getMarkdown()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Appendix - Data.World Names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keeping the names straight\n",
    "\n",
    "| CSV Name      | Table Name    | Title          | Dataset ID      | Restful |\n",
    "| :------------ |:------------- | :------------- | :-------------  | :------------- |\n",
    "| xxxx_xx       | xxxx_xx       | Xxxx Xx        | xxxx-xx         |    ?     | \n",
    "| xxxx_xx       | xxxx_xx       | Xxxx_Xx        | xxxxxx          |    ?            |\n",
    "| xxxx_xx       | xxxx_xx       | Xxxx-Xx        | xxxx-xx         |    ?         |\n",
    "| xxxx-xx       | xxxx_xx       | Xxxx Xx        | xxxx-xx         |    ?         |\n",
    "| xxxx-xx       | xxxx_xx       | Xxxx_Xx        | xxxxxx          |    ?         |\n",
    "| xxxx-xx       | xxxx_xx       | Xxxx-Xx        | xxxx-xx         |    ?         |\n",
    "\n",
    "* CSV Name is root of Table name\n",
    "* Title is root of Dataset ID\n",
    "* a space in Title will be automatically converted to hyphen in dataset id\n",
    "* an underscore in Title will be removed in Dataset ID\n",
    "* a hyphen in CSV Name will be replaced with underscore in Table Name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
